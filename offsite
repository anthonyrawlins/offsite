#!/bin/bash
set -euo pipefail

# Offsite - Unified ZFS Cloud Backup Tool v2.0
# Handles both backup and restore operations with ZFS-style syntax
# Features: Configurable shard sizes, streaming architecture, multi-terminal safety

# === USAGE ===
show_help() {
    cat << 'EOF'
Usage: offsite [--backup|--restore] <dataset[@backup]> [options]

OPERATIONS:
  --backup, -b    Backup a ZFS dataset to cloud storage
  --restore, -r   Restore a ZFS dataset from cloud storage
  --list, -l      List available backups and show completion status

ZFS-STYLE SYNTAX:
  dataset               - For backup: create auto-timestamped snapshot and backup
                         For restore: restore full dataset state (latest backup)
  dataset@snapshot      - For backup: create/use specific snapshot and backup
                         For restore: restore to specific backup point
  dataset@now           - For backup: create snapshot with timestamp
  dataset@today         - For backup: create snapshot with date
  dataset@latest        - For restore: same as dataset (ZFS semantics)
  dataset@latest-full   - For restore: most recent full backup only

BACKUP OPTIONS:
  --provider <name>     Provider to backup to (default: all)

RESTORE OPTIONS:
  --as <dataset>        Target dataset name (default: restore to original name)
  --provider <name>     Provider to restore from (default: auto-detect)

COMMON OPTIONS:
  --help, -h           Show this help
  --version, -v        Show version information

EXAMPLES:
  # Backup operations (always full backups)
  offsite --backup tank/important/data                    # Auto-timestamped snapshot
  offsite --backup tank/photos@now --provider scaleway    # Snapshot with timestamp
  offsite --backup tank/docs@backup-$(date +%Y%m%d)       # Custom snapshot name
  offsite --backup tank/projects@today                    # Snapshot with date
  
  # Restore operations
  offsite --restore tank/important/data                 # Restore to original name
  offsite --restore tank/photos@latest-full --as tank/photos-restored
  offsite --restore tank/docs@full-auto-20250716-161605 --as tank/docs-test

  # Short form (auto-detects operation from context)
  offsite tank/important/data                    # Backup (default operation)
  offsite tank/important/data@latest             # Restore (@ implies restore)
  
NOTE: --as flag is only available for restore operations in version 2.0+

PROVIDERS:
  all         - Backup to all configured providers
  auto        - Auto-detect provider (restore only)
  backblaze   - Backblaze B2 (US)
  scaleway    - Scaleway (EU)
  <custom>    - Any configured provider (e.g., amazonS3)

CONFIGURATION:
  Config file: ~/.config/offsite/config.env
  Setup: run setup-offsite to configure

EOF
}

# === ARGUMENT PARSING ===
OPERATION=""
DATASET_ARG=""
PROVIDER=""
AS_ARG=""
VERSION="2.0"

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --backup|-b)
            OPERATION="backup"
            shift
            ;;
        --restore|-r)
            OPERATION="restore"
            shift
            ;;
        --list|-l)
            OPERATION="list"
            shift
            ;;
        --provider)
            PROVIDER="$2"
            shift 2
            ;;
        --as)
            # Only allow --as for restore operations
            if [[ "$OPERATION" == "restore" ]]; then
                AS_ARG="$2"
                shift 2
            else
                echo "ERROR: --as flag only valid for restore operations"
                exit 1
            fi
            ;;
        --help|-h)
            show_help
            exit 0
            ;;
        --version|-v)
            echo "offsite version $VERSION"
            exit 0
            ;;
        -*)
            echo "ERROR: Unknown option $1"
            show_help
            exit 1
            ;;
        *)
            if [[ -z "$DATASET_ARG" ]]; then
                DATASET_ARG="$1"
            else
                echo "ERROR: Multiple datasets specified"
                show_help
                exit 1
            fi
            shift
            ;;
    esac
done

# Validate required arguments
if [[ -z "$DATASET_ARG" ]]; then
    echo "ERROR: No dataset specified"
    show_help
    exit 1
fi

# === AUTO-DETECT OPERATION ===
if [[ -z "$OPERATION" ]]; then
    if [[ "$DATASET_ARG" =~ @ ]]; then
        OPERATION="restore"
        echo "Auto-detected: restore operation (dataset@backup syntax)"
    else
        OPERATION="backup"
        echo "Auto-detected: backup operation (default)"
    fi
fi

# === LOAD CONFIG ===
CONFIG_FILE="${OFFSITE_CONFIG:-$HOME/.config/offsite/config.env}"
if [[ -f "$CONFIG_FILE" ]]; then
    source "$CONFIG_FILE"
else
    echo "WARNING: Config file not found at $CONFIG_FILE"
    echo "Using default values or environment variables"
fi

# === CONFIG WITH DEFAULTS ===
AGE_PUBLIC_KEY_FILE="${AGE_PUBLIC_KEY_FILE:-$HOME/.config/age/zfs-backup.pub}"
AGE_PRIVATE_KEY_FILE="${AGE_PRIVATE_KEY_FILE:-$HOME/.config/age/zfs-backup.txt}"
SNAP_PREFIX="${ZFS_SNAP_PREFIX:-auto}"
RETENTION_DAYS="${ZFS_RETENTION_DAYS:-14}"
TEMP_DIR="${TEMP_DIR:-/tmp}"
SHARD_SIZE="${SHARD_SIZE:-1G}"
VERSION="2.0"

# === DISCOVER PROVIDERS ===
declare -A PROVIDERS
declare -A PROVIDER_NAMES

# Built-in providers
PROVIDERS[backblaze]="${RCLONE_REMOTE:-cloudremote:zfs-buckets-walnut-deepblack-cloud/zfs-backups}"
PROVIDER_NAMES[backblaze]="Backblaze (US)"

PROVIDERS[scaleway]="${SCALEWAY_REMOTE:-scaleway:zfs-buckets-acacia/zfs-backups}"
PROVIDER_NAMES[scaleway]="Scaleway (EU)"

# Discover custom providers from config (any variable ending with _REMOTE)
if [[ -f "$CONFIG_FILE" ]]; then
    while IFS='=' read -r key value; do
        [[ "$key" =~ ^[A-Z_]+_REMOTE$ ]] && [[ "$key" != "RCLONE_REMOTE" ]] && [[ "$key" != "SCALEWAY_REMOTE" ]] && {
            provider_name=$(echo "$key" | sed 's/_REMOTE$//' | tr '[:upper:]' '[:lower:]')
            PROVIDERS[$provider_name]="$value"
            PROVIDER_NAMES[$provider_name]="$provider_name"
        }
    done < <(grep -E '^[A-Z_]+_REMOTE=' "$CONFIG_FILE" | sed 's/^export //')
fi

# === VALIDATION ===
command -v zfs >/dev/null || { echo "ERROR: zfs not installed"; exit 1; }
command -v rclone >/dev/null || { echo "ERROR: rclone not installed"; exit 1; }
command -v age >/dev/null || { echo "ERROR: age not installed"; exit 1; }

# Check for optional tools and suggest installation
if ! command -v pv >/dev/null 2>&1; then
    echo "INFO: pv (pipe viewer) not found - install for upload progress bars"
    echo "      Ubuntu/Debian: sudo apt install pv"
    echo "      RHEL/CentOS: sudo yum install pv"
fi

# === DELEGATE TO OPERATION ===
case "$OPERATION" in
    backup)
        # Source the backup functionality
        source <(sed -n '/^# === BACKUP FUNCTIONS ===/,/^# === END BACKUP FUNCTIONS ===/p' "$0")
        run_backup "$DATASET_ARG" "$PROVIDER"
        ;;
    restore)
        # Source the restore functionality
        source <(sed -n '/^# === RESTORE FUNCTIONS ===/,/^# === END RESTORE FUNCTIONS ===/p' "$0")
        run_restore "$DATASET_ARG" "$PROVIDER" "$AS_ARG"
        ;;
    list)
        # Source the list functionality  
        source <(sed -n '/^# === LIST FUNCTIONS ===/,/^# === END LIST FUNCTIONS ===/p' "$0")
        run_list "$DATASET_ARG" "$PROVIDER"
        ;;
    *)
        echo "ERROR: Invalid operation: $OPERATION"
        show_help
        exit 1
        ;;
esac

exit 0

# === BACKUP FUNCTIONS ===

run_backup() {
    local dataset_arg="$1"
    local provider="${2:-all}"
    
    # Parse dataset@snapshot syntax or use --as
    if [[ "$dataset_arg" =~ @ ]]; then
        local dataset="${dataset_arg%@*}"
        local snapshot_spec="${dataset_arg#*@}"
        
        
        # Handle special snapshot names
        case "$snapshot_spec" in
            today)
                snapshot_spec="${SNAP_PREFIX}-$(date +%Y%m%d)"
                ;;
            now)
                snapshot_spec="${SNAP_PREFIX}-$(date +%Y%m%d-%H%M%S)"
                ;;
        esac
        
        # Check if snapshot exists, if not create it
        if ! zfs list "${dataset}@${snapshot_spec}" >/dev/null 2>&1; then
            echo "Creating snapshot: ${dataset}@${snapshot_spec}"
            if ! zfs snapshot "${dataset}@${snapshot_spec}"; then
                echo "ERROR: Failed to create snapshot"
                exit 1
            fi
        fi
        
        BACKUP_DATASET="$dataset"
        BACKUP_SNAPSHOT="$snapshot_spec"
        echo "Backing up snapshot: ${dataset}@${snapshot_spec}"
    else
        # Create new snapshot
        BACKUP_DATASET="$dataset_arg"
        
        # Default snapshot name
        BACKUP_SNAPSHOT="${SNAP_PREFIX}-$(date +%Y%m%d-%H%M%S)"
        
        echo "Creating snapshot: ${BACKUP_DATASET}@${BACKUP_SNAPSHOT}"
        if ! zfs snapshot "${BACKUP_DATASET}@${BACKUP_SNAPSHOT}"; then
            echo "ERROR: Failed to create snapshot"
            exit 1
        fi
        
        # Wait for snapshot to be fully available
        echo "Waiting for snapshot to be fully available..."
        local retry_count=0
        while [[ $retry_count -lt 30 ]]; do
            if zfs list "${BACKUP_DATASET}@${BACKUP_SNAPSHOT}" >/dev/null 2>&1; then
                # Double-check snapshot is ready by trying to get its properties
                if zfs get -H -o value creation "${BACKUP_DATASET}@${BACKUP_SNAPSHOT}" >/dev/null 2>&1; then
                    echo "✓ Snapshot ready for backup"
                    break
                fi
            fi
            echo "  Waiting for snapshot to be ready... (${retry_count}/30)"
            sleep 1
            ((retry_count++))
        done
        
        if [[ $retry_count -eq 30 ]]; then
            echo "ERROR: Snapshot creation timed out or snapshot not accessible"
            exit 1
        fi
    fi
    
    # Validate dataset exists
    if ! zfs list "$BACKUP_DATASET" >/dev/null 2>&1; then
        echo "ERROR: Dataset '$BACKUP_DATASET' not found"
        exit 1
    fi
    
    # Validate age key
    if [[ ! -f "$AGE_PUBLIC_KEY_FILE" ]]; then
        echo "ERROR: Age public key not found at $AGE_PUBLIC_KEY_FILE"
        exit 1
    fi
    
    # Provider selection
    if [[ "$provider" == "all" ]]; then
        SELECTED_PROVIDERS=($(printf '%s\n' "${!PROVIDERS[@]}" | sort))
        echo "Selected providers: ${SELECTED_PROVIDERS[*]}"
    else
        # Handle legacy aliases
        case "$provider" in
            bb) provider="backblaze" ;;
            scw) provider="scaleway" ;;
        esac
        
        if [[ -z "${PROVIDERS[$provider]:-}" ]]; then
            echo "ERROR: Unknown provider '$provider'"
            echo "Available providers: ${!PROVIDERS[*]}"
            exit 1
        fi
        
        SELECTED_PROVIDERS=("$provider")
    fi
    
    # Test connections
    echo "Testing connections to selected providers..."
    for prov in "${SELECTED_PROVIDERS[@]}"; do
        remote="${PROVIDERS[$prov]}"
        echo "  Testing $prov (${PROVIDER_NAMES[$prov]})..."
        if ! rclone lsd "${remote%:*}:" >/dev/null 2>&1; then
            echo "ERROR: Cannot connect to $prov remote ${remote%:*}"
            exit 1
        fi
        echo "  ✓ Connected to $prov"
    done
    
    # Setup logging
    DATASET_LOG_FILE="$HOME/offsite-backup-$(echo "$BACKUP_DATASET" | tr '/' '_')-$(date +%Y%m%d-%H%M%S).log"
    # Simple logging without process substitution
    touch "$DATASET_LOG_FILE"
    
    # Create lockfile to prevent concurrent backups of same dataset
    LOCK_FILE="$HOME/.offsite-$(echo "$BACKUP_DATASET" | tr '/' '_').lock"
    
    # Check for existing lock
    if [[ -f "$LOCK_FILE" ]]; then
        local existing_pid=$(cat "$LOCK_FILE" 2>/dev/null || echo "")
        
        if [[ -z "$existing_pid" ]]; then
            echo "INFO: Removing corrupted lock file (no PID found)"
            rm -f "$LOCK_FILE"
        elif kill -0 "$existing_pid" 2>/dev/null; then
            # Process is still running - check if it's actually offsite
            local process_cmd=$(ps -p "$existing_pid" -o comm= 2>/dev/null || echo "")
            if [[ "$process_cmd" == *"offsite"* ]] || [[ "$process_cmd" == *"bash"* ]]; then
                echo "ERROR: Another offsite backup is already running for dataset $BACKUP_DATASET (PID $existing_pid)"
                echo "Process: $process_cmd"
                echo "If you're sure no other backup is running, remove: $LOCK_FILE"
                exit 1
            else
                echo "INFO: Removing stale lock file (PID $existing_pid is not offsite process: $process_cmd)"
                rm -f "$LOCK_FILE"
            fi
        else
            # PID doesn't exist - could be from reboot or killed process
            local lock_age=$(stat -c %Y "$LOCK_FILE" 2>/dev/null || echo "0")
            local current_time=$(date +%s)
            local age_minutes=$(( (current_time - lock_age) / 60 ))
            
            if [[ $age_minutes -gt 60 ]]; then
                echo "INFO: Removing old stale lock file (PID $existing_pid, ${age_minutes} minutes old - likely from reboot)"
            else
                echo "INFO: Removing stale lock file (PID $existing_pid no longer exists)"
            fi
            rm -f "$LOCK_FILE"
        fi
    fi
    
    # Create lock file with our PID
    echo $$ > "$LOCK_FILE"
    
    # Setup cleanup trap to remove lock file
    cleanup_lock() {
        rm -f "$LOCK_FILE"
    }
    trap cleanup_lock EXIT
    
    # Background-safe logging function
    log_message() {
        local message="$1"
        echo "$message" | tee -a "$DATASET_LOG_FILE"
    }
    
    # Check if running in background and inform user
    if [[ ! -t 1 ]]; then
        log_message "$(date '+%Y-%m-%d %H:%M:%S') INFO: Running in background mode"
        log_message "$(date '+%Y-%m-%d %H:%M:%S') INFO: Progress logged to $DATASET_LOG_FILE"
    fi
    
    echo "$(date '+%Y-%m-%d %H:%M:%S') === Offsite Backup Started ==="
    echo "$(date '+%Y-%m-%d %H:%M:%S') Dataset: $BACKUP_DATASET"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Snapshot: $BACKUP_SNAPSHOT"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Providers: ${SELECTED_PROVIDERS[*]}"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Backup type: full (always)"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Log file: $DATASET_LOG_FILE"
    
    # Backup to each provider
    BACKUP_START=$(date +%s)
    
    for prov in "${SELECTED_PROVIDERS[@]}"; do
        rclone_remote="${PROVIDERS[$prov]}"
        provider_name="${PROVIDER_NAMES[$prov]}"
        backup_single_provider "$BACKUP_DATASET" "$BACKUP_SNAPSHOT" "$prov" "$rclone_remote" "$provider_name"
    done
    
    # Cleanup old snapshots
    cleanup_old_snapshots "$BACKUP_DATASET"
    
    BACKUP_END=$(date +%s)
    TOTAL_TIME=$((BACKUP_END - BACKUP_START))
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')"
    echo "$(date '+%Y-%m-%d %H:%M:%S') === Offsite Backup Completed ==="
    echo "$(date '+%Y-%m-%d %H:%M:%S') Dataset: $BACKUP_DATASET"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Snapshot: $BACKUP_SNAPSHOT"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Providers: ${SELECTED_PROVIDERS[*]}"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Total time: ${TOTAL_TIME}s ($(($TOTAL_TIME / 60))m $(($TOTAL_TIME % 60))s)"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Log saved to: $DATASET_LOG_FILE"
}

backup_single_provider() {
    local dataset="$1"
    local snap="$2"
    local provider="$3"
    local rclone_remote="$4"
    local provider_name="$5"
    local dataset_path="${dataset//\//_}"
    
    echo "$(date '+%Y-%m-%d %H:%M:%S') [+] Backing up ${dataset}@${snap} to $provider_name"
    
    # Always perform full backups - simpler and more reliable
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Performing full backup"
    
    # No working directory needed for streaming backup!
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Using streaming backup - no temp files required"
    
    # Get trimmed age public key
    local agekey=$(cat "$AGE_PUBLIC_KEY_FILE" | tr -d '\n')
    
    # Always do full backup
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Full backup"
    local prefix="full"
    local backup_prefix="${prefix}-${snap}"
    
    stream_zfs_to_cloud "" "${dataset}@${snap}" "$backup_prefix" "$rclone_remote" "$dataset_path" "$agekey" "full"
    
    # No cleanup needed - streaming backup used no temp files!
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')     ✓ Backup complete to $provider_name"
}

# Check if a backup is complete by looking for shard 100
is_backup_complete() {
    local rclone_remote="$1"
    local dataset_path="$2" 
    local backup_prefix="$3"
    
    local completion_shard="${backup_prefix}-s100.zfs.gz.age"
    rclone lsf "${rclone_remote}/${dataset_path}/" 2>/dev/null | grep -q "^${completion_shard}$"
}

# Streaming ZFS backup function - no temp files needed!
stream_zfs_to_cloud() {
    local prev_snap="$1"
    local current_snap="$2"
    local backup_prefix="$3"
    local rclone_remote="$4"
    local dataset_path="$5"
    local agekey="$6"
    local backup_type="$7"
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Starting streaming backup ($backup_type)"
    
    # Check for existing shards (resumable backup support)
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Checking for existing shards..."
    local existing_shards=($(rclone lsf "${rclone_remote}/${dataset_path}/" 2>/dev/null | grep "^${backup_prefix}-s[0-9][0-9][0-9].zfs.gz.age$" | sort || true))
    
    if [[ ${#existing_shards[@]} -gt 0 ]]; then
        # Check if backup is already complete by looking for shard 100
        if is_backup_complete "$rclone_remote" "$dataset_path" "$backup_prefix"; then
            echo "$(date '+%Y-%m-%d %H:%M:%S')     ✓ Backup already complete: found shard 100 of 100"
            echo "$(date '+%Y-%m-%d %H:%M:%S')     ✓ All ${#existing_shards[@]} shards exist in cloud storage"
            echo "$(date '+%Y-%m-%d %H:%M:%S')     Skipping backup - already complete"
            return 0
        else
            echo "$(date '+%Y-%m-%d %H:%M:%S')     Found ${#existing_shards[@]} existing shards - resuming backup"
            echo "$(date '+%Y-%m-%d %H:%M:%S')       Existing: ${existing_shards[0]} ... ${existing_shards[-1]}"
        fi
    else
        echo "$(date '+%Y-%m-%d %H:%M:%S')     No existing shards found - starting fresh backup"
    fi
    
    # Create the zfs send command - always full
    local zfs_cmd="zfs send \"$current_snap\""
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Streaming: $zfs_cmd"
    
    # Estimate number of shards based on dataset size
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Estimating backup size..."
    local dataset_size_bytes=0
    local estimated_shards=0
    
    # Get dataset size for progress estimation (always full backup)
    dataset_size_bytes=$(zfs get -H -o value -p used "${current_snap%@*}" 2>/dev/null || echo "1073741824")
    
    # Calculate dynamic shard size: 1% of dataset = exactly 100 shards
    # This provides perfect progress tracking regardless of dataset size
    local dynamic_shard_size_bytes=$((dataset_size_bytes / 100))
    
    # Set minimum shard size (10MB) and maximum shard size (50GB) for practical limits
    local min_shard_size=$((10 * 1024 * 1024))      # 10MB minimum
    local max_shard_size=$((50 * 1024 * 1024 * 1024)) # 50GB maximum
    
    if [[ $dynamic_shard_size_bytes -lt $min_shard_size ]]; then
        dynamic_shard_size_bytes=$min_shard_size
        estimated_shards=$(((dataset_size_bytes + dynamic_shard_size_bytes - 1) / dynamic_shard_size_bytes))
    elif [[ $dynamic_shard_size_bytes -gt $max_shard_size ]]; then
        dynamic_shard_size_bytes=$max_shard_size
        estimated_shards=$(((dataset_size_bytes + dynamic_shard_size_bytes - 1) / dynamic_shard_size_bytes))
    else
        estimated_shards=100  # Exactly 100 shards at 1% each
    fi
    
    estimated_shards=$((estimated_shards > 0 ? estimated_shards : 1))  # Minimum 1 shard
    
    # Convert to human-readable shard size
    local shard_size_human
    if [[ $dynamic_shard_size_bytes -ge $((1024 * 1024 * 1024)) ]]; then
        shard_size_human="$((dynamic_shard_size_bytes / 1024 / 1024 / 1024))G"
    elif [[ $dynamic_shard_size_bytes -ge $((1024 * 1024)) ]]; then
        shard_size_human="$((dynamic_shard_size_bytes / 1024 / 1024))M"
    else
        shard_size_human="$((dynamic_shard_size_bytes / 1024))K"
    fi
    
    # Show size estimation
    local size_mb=$((dataset_size_bytes / 1024 / 1024))
    if [[ $size_mb -gt 1024 ]]; then
        local size_gb=$((size_mb / 1024))
        echo "$(date '+%Y-%m-%d %H:%M:%S')     Estimated size: ${size_gb}GB (${estimated_shards} shards of ${shard_size_human} each)"
    else
        echo "$(date '+%Y-%m-%d %H:%M:%S')     Estimated size: ${size_mb}MB (${estimated_shards} shards of ${shard_size_human} each)"
    fi
    
    # True streaming approach: process shards one at a time as split creates them
    local temp_dir=$(mktemp -d)
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Temp directory: $temp_dir"
    
    # Cleanup function
    cleanup_temp_files() {
        rm -rf "$temp_dir" 2>/dev/null || true
    }
    trap cleanup_temp_files EXIT
    
    # Use a more direct approach: monitor split output directory and process shards as they complete
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Starting streaming split and upload pipeline..."
    
    # Start split in background but monitor for complete shards
    eval "$zfs_cmd" | split -b "$dynamic_shard_size_bytes" --numeric-suffixes=1 --suffix-length=3 - "$temp_dir/shard-" &
    local split_pid=$!
    
    local shard_num=1
    local processed_count=0
    local upload_success=true
    
    # Monitor for shards and process them as they become available
    while kill -0 $split_pid 2>/dev/null || [[ -n "$(ls "$temp_dir"/shard-* 2>/dev/null)" ]]; do
        # Look for the next expected shard
        local expected_shard=$(printf "%s/shard-%03d" "$temp_dir" "$shard_num")
        
        if [[ -f "$expected_shard" ]]; then
            # Wait a moment to ensure the shard is complete (split might still be writing to it)
            local initial_size=$(stat -c%s "$expected_shard" 2>/dev/null || echo "0")
            sleep 0.1
            local final_size=$(stat -c%s "$expected_shard" 2>/dev/null || echo "0")
            
            # If the file is still growing or split is still running on this shard, wait
            if [[ $initial_size -ne $final_size ]] && kill -0 $split_pid 2>/dev/null; then
                sleep 0.1
                continue
            fi
            
            # Process this shard
            local shard_suffix=$(printf "%03d" "$shard_num")
            local output_file="${backup_prefix}-s${shard_suffix}.zfs.gz.age"
            
            # Check if shard already exists (resumable backup)
            if printf '%s\n' "${existing_shards[@]}" | grep -q "^${output_file}$"; then
                echo "$(date '+%Y-%m-%d %H:%M:%S')       ✓ Shard $shard_num already exists: $output_file"
            else
                echo "$(date '+%Y-%m-%d %H:%M:%S')       Processing shard $shard_num..."
                
                # Get shard file size for progress bar
                local shard_size=$(stat -c%s "$expected_shard" 2>/dev/null || echo "0")
                
                # Stream shard: compress -> encrypt -> upload (no progress bar for testing)
                echo "$(date '+%Y-%m-%d %H:%M:%S')         Uploading $(numfmt --to=iec $shard_size)..."
                if gzip < "$expected_shard" | age -r "$agekey" | rclone rcat "${rclone_remote}/${dataset_path}/${output_file}"; then
                    echo "$(date '+%Y-%m-%d %H:%M:%S')         ✓ Uploaded $output_file"
                else
                    echo "$(date '+%Y-%m-%d %H:%M:%S')         ✗ Failed to upload $output_file"
                    upload_success=false
                    break
                fi
            fi
            
            # Clean up temp file immediately after processing
            rm -f "$expected_shard"
            ((processed_count++))
            ((shard_num++))
        else
            # No shard ready yet, wait a bit
            sleep 0.1
        fi
    done
    
    # Wait for split to complete
    wait $split_pid
    split_exit_code=$?
    
    # Process any remaining shards that split created after our last check
    for remaining_shard in "$temp_dir"/shard-*; do
        [[ -f "$remaining_shard" ]] || continue
        
        local remaining_suffix=$(basename "$remaining_shard" | sed 's/shard-//')
        local output_file="${backup_prefix}-s${remaining_suffix}.zfs.gz.age"
        
        echo "$(date '+%Y-%m-%d %H:%M:%S')       Processing final shard..."
        
        # Stream shard: compress -> encrypt -> upload (no progress bar for testing)
        local shard_size=$(stat -c%s "$remaining_shard" 2>/dev/null || echo "0")
        echo "$(date '+%Y-%m-%d %H:%M:%S')         Uploading $(numfmt --to=iec $shard_size)..."
        if gzip < "$remaining_shard" | age -r "$agekey" | rclone rcat "${rclone_remote}/${dataset_path}/${output_file}"; then
            echo "$(date '+%Y-%m-%d %H:%M:%S')         ✓ Uploaded $output_file"
        else
            echo "$(date '+%Y-%m-%d %H:%M:%S')         ✗ Failed to upload $output_file"
            upload_success=false
        fi
        
        rm -f "$remaining_shard"
        ((processed_count++))
    done
    
    # Clean up temp directory
    cleanup_temp_files
    trap - EXIT
    
    if [[ "$upload_success" == "true" ]] && [[ $split_exit_code -eq 0 ]]; then
        echo "$(date '+%Y-%m-%d %H:%M:%S')     ✓ Streaming backup complete: $processed_count shards uploaded"
        return 0
    else
        echo "$(date '+%Y-%m-%d %H:%M:%S')     ✗ Backup failed (split exit: $split_exit_code, upload success: $upload_success)"
        return 1
    fi
}

cleanup_old_snapshots() {
    local dataset="$1"
    echo "$(date '+%Y-%m-%d %H:%M:%S') [+] Cleaning up old snapshots for $dataset"
    
    zfs list -H -t snapshot -o name,creation -s creation | grep "^${dataset}@${SNAP_PREFIX}-" | while read snap creation; do
        local snap_time=$(echo "$snap" | awk -F@ '{print $2}' | sed 's/.*-//')
        local snap_date=$(date -d "${snap_time:0:8} ${snap_time:8:2}:${snap_time:10:2}:${snap_time:12:2}" +%s 2>/dev/null || continue)
        local now=$(date +%s)
        local age_days=$(( (now - snap_date) / 86400 ))
        
        if (( age_days > RETENTION_DAYS )); then
            echo "$(date '+%Y-%m-%d %H:%M:%S')     Destroying old snapshot $snap (${age_days} days old)"
            zfs destroy "$snap"
        fi
    done
}

# === END BACKUP FUNCTIONS ===

# === RESTORE FUNCTIONS ===

run_restore() {
    local dataset_arg="$1"
    local provider="${2:-auto}"
    local as_arg="$3"
    
    # Parse dataset@backup syntax
    if [[ "$dataset_arg" =~ @ ]]; then
        local source_dataset="${dataset_arg%@*}"
        local backup_prefix="${dataset_arg#*@}"
        
        # ZFS semantics: dataset and dataset@latest are identical
        if [[ "$backup_prefix" == "latest" ]]; then
            echo "Note: dataset@latest is the same as dataset (full dataset state)"
        fi
    else
        local source_dataset="$dataset_arg"
        local backup_prefix="latest"
    fi
    
    # Set target dataset from --as or use original name
    local target_dataset
    if [[ -n "$as_arg" ]]; then
        target_dataset="$as_arg"
    else
        target_dataset="$source_dataset"
    fi
    
    # Validate age key
    if [[ ! -f "$AGE_PRIVATE_KEY_FILE" ]]; then
        echo "ERROR: Age private key not found at $AGE_PRIVATE_KEY_FILE"
        exit 1
    fi
    
    # Provider selection
    case "$provider" in
        auto)
            # Try all providers, prefer backblaze
            echo "Auto mode: Testing all providers..."
            local dataset_path="${source_dataset//\//_}"
            local selected_remote=""
            local provider_name=""
            
            # Try providers in order of preference
            for prov in backblaze scaleway $(printf '%s\n' "${!PROVIDERS[@]}" | grep -v -E '^(backblaze|scaleway)$' | sort); do
                local remote="${PROVIDERS[$prov]}"
                if rclone lsf "${remote}/${dataset_path}/" 2>/dev/null | grep -q "^.*-x001.zfs.gz.age$"; then
                    selected_remote="$remote"
                    provider_name="${PROVIDER_NAMES[$prov]} - auto-selected"
                    echo "Found backups on $prov"
                    break
                fi
            done
            
            if [[ -z "$selected_remote" ]]; then
                echo "ERROR: No backups found on any provider"
                echo "Available providers: ${!PROVIDERS[*]}"
                exit 1
            fi
            ;;
        *)
            # Handle legacy aliases
            case "$provider" in
                bb) provider="backblaze" ;;
                scw) provider="scaleway" ;;
            esac
            
            if [[ -z "${PROVIDERS[$provider]:-}" ]]; then
                echo "ERROR: Unknown provider '$provider'"
                echo "Available providers: ${!PROVIDERS[*]}"
                exit 1
            fi
            
            selected_remote="${PROVIDERS[$provider]}"
            provider_name="${PROVIDER_NAMES[$provider]}"
            ;;
    esac
    
    # Setup variables
    local dataset_path="${source_dataset//\//_}"
    local remote_path="${selected_remote}/${dataset_path}"
    
    echo "=== Offsite Restore ==="
    echo "Provider: $provider_name"
    echo "Source dataset: $source_dataset"
    echo "Target dataset: $target_dataset"
    echo "Remote path: $remote_path"
    echo ""
    
    # Backup selection
    if [[ "$backup_prefix" == "latest" ]] || [[ "$backup_prefix" == "latest-full" ]]; then
        echo "Discovering available backups..."
        
        # Get all backup prefixes for this dataset
        local all_backups=($(rclone lsf "${remote_path}/" 2>/dev/null | grep -E '^(full|incr)-.*-x[0-9][0-9][0-9]\.zfs\.gz\.age$' | sed 's/-x[0-9][0-9][0-9]\.zfs\.gz\.age$//' | sort -u | sort -t- -k3,3 || true))
        
        if [[ ${#all_backups[@]} -eq 0 ]]; then
            echo "ERROR: No backups found for dataset $source_dataset"
            exit 1
        fi
        
        if [[ "$backup_prefix" == "latest-full" ]]; then
            # Find latest full backup
            local selected_backup=""
            for backup in "${all_backups[@]}"; do
                if [[ "$backup" =~ ^full- ]]; then
                    selected_backup="$backup"
                fi
            done
            
            if [[ -z "$selected_backup" ]]; then
                echo "ERROR: No full backups found for dataset $source_dataset"
                exit 1
            fi
            
            backup_prefix="$selected_backup"
            echo "Selected latest full backup: $backup_prefix"
        else
            # Find latest backup (incremental preferred)
            backup_prefix="${all_backups[-1]}"
            echo "Selected latest backup: $backup_prefix"
        fi
        
        echo "Available backups for $source_dataset:"
        printf '%s\n' "${all_backups[@]}" | tail -5
        echo ""
    else
        # User specified a specific backup - need to find the actual backup prefix
        echo "Searching for backup matching: $backup_prefix"
        
        # Get all backup prefixes for this dataset
        local all_backups=($(rclone lsf "${remote_path}/" 2>/dev/null | grep -E '^(full|incr)-.*-x[0-9][0-9][0-9]\.zfs\.gz\.age$' | sed 's/-x[0-9][0-9][0-9]\.zfs\.gz\.age$//' | sort -u | sort -t- -k3,3 || true))
        
        if [[ ${#all_backups[@]} -eq 0 ]]; then
            echo "ERROR: No backups found for dataset $source_dataset"
            exit 1
        fi
        
        # Look for backup that ends with the specified snapshot name
        local found_backup=""
        for backup in "${all_backups[@]}"; do
            if [[ "$backup" == "full-${backup_prefix}" ]] || [[ "$backup" == "incr-${backup_prefix}" ]]; then
                found_backup="$backup"
                break
            fi
        done
        
        if [[ -z "$found_backup" ]]; then
            echo "ERROR: No backup found matching snapshot: $backup_prefix"
            echo "Available backups:"
            printf '%s\n' "${all_backups[@]}"
            exit 1
        fi
        
        backup_prefix="$found_backup"
        echo "Found matching backup: $backup_prefix"
    fi
    
    echo "Final backup selection: $backup_prefix"
    echo ""
    
    # Discover shards
    echo "Discovering backup shards..."
    local shard_files=($(rclone lsf "${remote_path}/" 2>/dev/null | grep "^${backup_prefix}-s[0-9][0-9][0-9].zfs.gz.age$" | sort))
    
    if [[ ${#shard_files[@]} -eq 0 ]]; then
        echo "ERROR: No shards found for backup prefix: ${backup_prefix}"
        echo ""
        echo "Available backups for ${source_dataset}:"
        rclone lsf "${remote_path}/" 2>/dev/null | grep -E '^(full|incr)-.*-x[0-9][0-9][0-9]\.zfs\.gz\.age$' | sed 's/-x[0-9][0-9][0-9]\.zfs\.gz\.age$//' | sort -u || echo "No backups found"
        exit 1
    fi
    
    echo "Found ${#shard_files[@]} shards: ${shard_files[0]} ... ${shard_files[-1]}"
    
    # Connection test
    echo "Testing connection to $provider_name..."
    if ! rclone lsd "${selected_remote%:*}:" >/dev/null 2>&1; then
        echo "ERROR: Cannot connect to $provider_name"
        exit 1
    fi
    echo "✓ Connected to $provider_name"
    
    # Target dataset validation
    if zfs list "$target_dataset" >/dev/null 2>&1; then
        # Check if it's a pool or dataset
        local target_type=$(zfs get -H -o value type "$target_dataset" 2>/dev/null)
        
        if [[ "$target_type" == "filesystem" ]]; then
            echo "WARNING: Target dataset $target_dataset already exists"
            
            # Check if running in background (no TTY)
            if [[ ! -t 0 ]]; then
                echo "ERROR: Cannot prompt for confirmation when running in background"
                echo "Target dataset already exists. Use --as to specify a different target name"
                exit 1
            fi
            
            read -p "Do you want to destroy and recreate it? (y/N): " -n 1 -r
            echo
            if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                echo "Restore cancelled"
                echo "Suggestion: Use --as to specify a different target name"
                exit 1
            fi
            echo "Destroying existing dataset..."
            zfs destroy -r "$target_dataset"
        else
            echo "ERROR: Target $target_dataset exists but is not a filesystem"
            echo "Type: $target_type"
            echo "Use --as to specify a different target name"
            exit 1
        fi
    else
        # Check if parent exists for dataset creation
        local parent_dataset=$(dirname "$target_dataset" | sed 's|/.*||' | head -1)
        local dataset_path=$(echo "$target_dataset" | sed 's|^[^/]*/||')
        
        # If target contains '/', check parent exists
        if [[ "$target_dataset" =~ / ]]; then
            local parent_path=$(echo "$target_dataset" | sed 's|/[^/]*$||')
            if ! zfs list "$parent_path" >/dev/null 2>&1; then
                echo "ERROR: Parent dataset $parent_path does not exist"
                echo "Create parent first or use --as with an existing parent"
                exit 1
            fi
        fi
    fi
    
    # Restore process
    local work_dir="${TEMP_DIR}/offsite-restore-$(date +%s)"
    mkdir -p "$work_dir"
    cd "$work_dir"
    
    echo ""
    echo "Starting restore process..."
    echo "Working directory: $work_dir"
    
    # Download all shards
    echo "Step 1: Downloading shards..."
    for shard_file in "${shard_files[@]}"; do
        echo "  Downloading: $shard_file"
        rclone copy "${remote_path}/${shard_file}" ./
    done
    
    # Decrypt and decompress shards
    echo "Step 2: Decrypting and decompressing shards..."
    for shard_file in "${shard_files[@]}"; do
        echo "  Processing: $shard_file"
        # Extract the shard number from filename
        local shard_num=$(echo "$shard_file" | sed 's/.*-s\([0-9][0-9][0-9]\)\.zfs\.gz\.age$/\1/')
        age -d -i "$AGE_PRIVATE_KEY_FILE" "$shard_file" | gunzip > "shard-${shard_num}.restored"
        echo "    ✓ Restored: shard-${shard_num}.restored"
    done
    
    # Reconstitute stream
    echo "Step 3: Reconstituting ZFS stream..."
    cat shard-*.restored > complete-stream.zfs
    echo "Complete stream size: $(wc -c < complete-stream.zfs) bytes"
    
    # ZFS receive
    echo "Step 4: Performing ZFS receive..."
    zfs recv -F "$target_dataset" < complete-stream.zfs
    
    # Cleanup
    cd /
    rm -rf "$work_dir"
    
    echo ""
    echo "✅ Restore complete!"
    echo ""
    echo "Source: $source_dataset ($backup_prefix)"
    echo "Target: $target_dataset"
    echo "Shards processed: ${#shard_files[@]}"
    echo "Provider: $provider_name"
    echo ""
    echo "You can now:"
    echo "  - Mount: zfs mount $target_dataset"
    echo "  - List snapshots: zfs list -t snapshot -r $target_dataset"
    echo "  - Access data at: $(zfs get -H -o value mountpoint $target_dataset 2>/dev/null || echo 'mount point')"
}

# === END RESTORE FUNCTIONS ===

# === LIST FUNCTIONS ===

run_list() {
    local dataset_arg="$1"
    local provider="${2:-all}"
    
    echo "=== Offsite Backup List ==="
    echo ""
    
    # Provider selection (same logic as backup)
    if [[ "$provider" == "all" ]]; then
        SELECTED_PROVIDERS=($(printf '%s\n' "${\!PROVIDERS[@]}"  < /dev/null |  sort))
    else
        case "$provider" in
            bb) provider="backblaze" ;;
            scw) provider="scaleway" ;;
        esac
        
        if [[ -z "${PROVIDERS[$provider]:-}" ]]; then
            echo "ERROR: Unknown provider '$provider'"
            echo "Available providers: ${\!PROVIDERS[*]}"
            exit 1
        fi
        
        SELECTED_PROVIDERS=("$provider")
    fi
    
    # List backups for each provider
    for prov in "${SELECTED_PROVIDERS[@]}"; do
        rclone_remote="${PROVIDERS[$prov]}"
        provider_name="${PROVIDER_NAMES[$prov]}"
        
        echo "📡 $provider_name ($prov)"
        echo "─────────────────────────────────────"
        
        if [[ -n "$dataset_arg" ]]; then
            # List specific dataset
            list_dataset_backups "$rclone_remote" "$dataset_arg"
        else
            # List all datasets
            list_all_backups "$rclone_remote"
        fi
        
        echo ""
    done
}

list_dataset_backups() {
    local rclone_remote="$1"
    local dataset="$2"
    local dataset_path="${dataset//\//_}"
    
    # Get all backup prefixes for this dataset
    local backup_files=($(rclone lsf "${rclone_remote}/${dataset_path}/" 2>/dev/null | grep -E "^(full|incr)-.*-s[0-9][0-9][0-9]\.zfs\.gz\.age$" | sort || true))
    
    if [[ ${#backup_files[@]} -eq 0 ]]; then
        echo "  No backups found for $dataset"
        return
    fi
    
    # Extract unique backup prefixes
    local backup_prefixes=($(printf '%s\n' "${backup_files[@]}" | sed -E 's/-s[0-9][0-9][0-9]\.zfs\.gz\.age$//' | sort -u))
    
    echo "  Dataset: $dataset"
    echo ""
    
    for prefix in "${backup_prefixes[@]}"; do
        # Count shards for this backup
        local shard_count=$(printf '%s\n' "${backup_files[@]}" | grep "^${prefix}-s" | wc -l)
        
        # Check if backup is complete (has shard 100)
        local status="❌ Incomplete"
        if is_backup_complete "$rclone_remote" "$dataset_path" "$prefix"; then
            status="✅ Complete"
        fi
        
        # Extract backup info from prefix
        local backup_type="${prefix%%-*}"
        local backup_date="${prefix#*-}"
        
        echo "    $status $backup_type-$backup_date ($shard_count shards)"
    done
}

list_all_backups() {
    local rclone_remote="$1"
    
    # Get all dataset directories
    local datasets=($(rclone lsd "${rclone_remote}/" 2>/dev/null | awk '{print $NF}' | grep -v "^$" || true))
    
    if [[ ${#datasets[@]} -eq 0 ]]; then
        echo "  No backups found"
        return
    fi
    
    for dataset_path in "${datasets[@]}"; do
        # Convert path back to dataset name
        local dataset="${dataset_path//_//}"
        list_dataset_backups "$rclone_remote" "$dataset"
        echo ""
    done
}

# === END LIST FUNCTIONS ===
