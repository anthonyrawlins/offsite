#!/bin/bash
set -euo pipefail

# Offsite - Unified ZFS Cloud Backup Tool v2.0
# Handles both backup and restore operations with ZFS-style syntax
# Features: Configurable shard sizes, streaming architecture, multi-terminal safety

# === USAGE ===
show_help() {
    cat << 'EOF'
Usage: offsite [--backup|--restore] <dataset[@backup]> [options]

OPERATIONS:
  --backup, -b    Backup a ZFS dataset to cloud storage
  --restore, -r   Restore a ZFS dataset from cloud storage

ZFS-STYLE SYNTAX:
  dataset               - For backup: create snapshot and backup
                         For restore: restore full dataset state (latest backup)
  dataset@snapshot      - For backup: backup specific snapshot
                         For restore: restore to specific backup point
  dataset@latest        - For restore: same as dataset (ZFS semantics)
  dataset@latest-full   - For restore: most recent full backup only

BACKUP OPTIONS:
  --as @snapshot        Snapshot name to create (e.g., @today, @now, @custom-name)
  --provider <name>     Provider to backup to (default: all)
  --full               Force full backup (no incremental)

RESTORE OPTIONS:
  --as <dataset>        Target dataset name (default: restore to original name)
  --provider <name>     Provider to restore from (default: auto-detect)

COMMON OPTIONS:
  --help, -h           Show this help
  --version, -v        Show version information

EXAMPLES:
  # Backup operations
  offsite --backup tank/important/data
  offsite --backup tank/photos --as @today --provider scaleway
  offsite --backup tank/docs --as @backup-$(date +%Y%m%d) --full
  
  # Restore operations
  offsite --restore tank/important/data                 # Restore to original name
  offsite --restore tank/photos@latest-full --as tank/photos-restored
  offsite --restore tank/docs@full-auto-20250716-161605 --as tank/docs-test

  # Short form (auto-detects operation from context)
  offsite tank/important/data                    # Backup (default operation)
  offsite tank/important/data@latest             # Restore (@ implies restore)

PROVIDERS:
  all         - Backup to all configured providers
  auto        - Auto-detect provider (restore only)
  backblaze   - Backblaze B2 (US)
  scaleway    - Scaleway (EU)
  <custom>    - Any configured provider (e.g., amazonS3)

CONFIGURATION:
  Config file: ~/.config/offsite/config.env
  Setup: run setup-offsite to configure

EOF
}

# === ARGUMENT PARSING ===
OPERATION=""
DATASET_ARG=""
PROVIDER=""
AS_ARG=""
FORCE_FULL=false
VERSION="1.0.0"

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --backup|-b)
            OPERATION="backup"
            shift
            ;;
        --restore|-r)
            OPERATION="restore"
            shift
            ;;
        --provider)
            PROVIDER="$2"
            shift 2
            ;;
        --as)
            AS_ARG="$2"
            shift 2
            ;;
        --full)
            FORCE_FULL=true
            shift
            ;;
        --help|-h)
            show_help
            exit 0
            ;;
        --version|-v)
            echo "offsite version $VERSION"
            exit 0
            ;;
        -*)
            echo "ERROR: Unknown option $1"
            show_help
            exit 1
            ;;
        *)
            if [[ -z "$DATASET_ARG" ]]; then
                DATASET_ARG="$1"
            else
                echo "ERROR: Multiple datasets specified"
                show_help
                exit 1
            fi
            shift
            ;;
    esac
done

# Validate required arguments
if [[ -z "$DATASET_ARG" ]]; then
    echo "ERROR: No dataset specified"
    show_help
    exit 1
fi

# === AUTO-DETECT OPERATION ===
if [[ -z "$OPERATION" ]]; then
    if [[ "$DATASET_ARG" =~ @ ]]; then
        OPERATION="restore"
        echo "Auto-detected: restore operation (dataset@backup syntax)"
    else
        OPERATION="backup"
        echo "Auto-detected: backup operation (default)"
    fi
fi

# === LOAD CONFIG ===
CONFIG_FILE="${OFFSITE_CONFIG:-$HOME/.config/offsite/config.env}"
if [[ -f "$CONFIG_FILE" ]]; then
    source "$CONFIG_FILE"
else
    echo "WARNING: Config file not found at $CONFIG_FILE"
    echo "Using default values or environment variables"
fi

# === CONFIG WITH DEFAULTS ===
AGE_PUBLIC_KEY_FILE="${AGE_PUBLIC_KEY_FILE:-$HOME/.config/age/zfs-backup.pub}"
AGE_PRIVATE_KEY_FILE="${AGE_PRIVATE_KEY_FILE:-$HOME/.config/age/zfs-backup.txt}"
SNAP_PREFIX="${ZFS_SNAP_PREFIX:-auto}"
RETENTION_DAYS="${ZFS_RETENTION_DAYS:-14}"
TEMP_DIR="${TEMP_DIR:-/tmp}"
SHARD_SIZE="${SHARD_SIZE:-1G}"
VERSION="2.0"

# === DISCOVER PROVIDERS ===
declare -A PROVIDERS
declare -A PROVIDER_NAMES

# Built-in providers
PROVIDERS[backblaze]="${RCLONE_REMOTE:-cloudremote:zfs-buckets-walnut-deepblack-cloud/zfs-backups}"
PROVIDER_NAMES[backblaze]="Backblaze (US)"

PROVIDERS[scaleway]="${SCALEWAY_REMOTE:-scaleway:zfs-buckets-acacia/zfs-backups}"
PROVIDER_NAMES[scaleway]="Scaleway (EU)"

# Discover custom providers from config (any variable ending with _REMOTE)
if [[ -f "$CONFIG_FILE" ]]; then
    while IFS='=' read -r key value; do
        [[ "$key" =~ ^[A-Z_]+_REMOTE$ ]] && [[ "$key" != "RCLONE_REMOTE" ]] && [[ "$key" != "SCALEWAY_REMOTE" ]] && {
            provider_name=$(echo "$key" | sed 's/_REMOTE$//' | tr '[:upper:]' '[:lower:]')
            PROVIDERS[$provider_name]="$value"
            PROVIDER_NAMES[$provider_name]="$provider_name"
        }
    done < <(grep -E '^[A-Z_]+_REMOTE=' "$CONFIG_FILE" | sed 's/^export //')
fi

# === VALIDATION ===
command -v zfs >/dev/null || { echo "ERROR: zfs not installed"; exit 1; }
command -v rclone >/dev/null || { echo "ERROR: rclone not installed"; exit 1; }
command -v age >/dev/null || { echo "ERROR: age not installed"; exit 1; }

# === DELEGATE TO OPERATION ===
case "$OPERATION" in
    backup)
        # Source the backup functionality
        source <(sed -n '/^# === BACKUP FUNCTIONS ===/,/^# === END BACKUP FUNCTIONS ===/p' "$0")
        run_backup "$DATASET_ARG" "$PROVIDER" "$AS_ARG" "$FORCE_FULL"
        ;;
    restore)
        # Source the restore functionality
        source <(sed -n '/^# === RESTORE FUNCTIONS ===/,/^# === END RESTORE FUNCTIONS ===/p' "$0")
        run_restore "$DATASET_ARG" "$PROVIDER" "$AS_ARG"
        ;;
    *)
        echo "ERROR: Invalid operation: $OPERATION"
        show_help
        exit 1
        ;;
esac

exit 0

# === BACKUP FUNCTIONS ===

run_backup() {
    local dataset_arg="$1"
    local provider="${2:-all}"
    local as_arg="$3"
    local force_full="$4"
    
    # Parse dataset@snapshot syntax or use --as
    if [[ "$dataset_arg" =~ @ ]]; then
        local dataset="${dataset_arg%@*}"
        local snapshot_spec="${dataset_arg#*@}"
        
        if [[ -n "$as_arg" ]]; then
            echo "ERROR: Cannot specify both dataset@snapshot and --as"
            exit 1
        fi
        
        # Handle special snapshot names
        case "$snapshot_spec" in
            today)
                snapshot_spec="${SNAP_PREFIX}-$(date +%Y%m%d)"
                ;;
            now)
                snapshot_spec="${SNAP_PREFIX}-$(date +%Y%m%d-%H%M%S)"
                ;;
        esac
        
        # Check if snapshot exists
        if ! zfs list "${dataset}@${snapshot_spec}" >/dev/null 2>&1; then
            echo "ERROR: Snapshot ${dataset}@${snapshot_spec} does not exist"
            echo "Available snapshots:"
            zfs list -t snapshot -o name "${dataset}" | head -10
            exit 1
        fi
        
        BACKUP_DATASET="$dataset"
        BACKUP_SNAPSHOT="$snapshot_spec"
        echo "Backing up existing snapshot: ${dataset}@${snapshot_spec}"
    else
        # Create new snapshot
        BACKUP_DATASET="$dataset_arg"
        
        if [[ -n "$as_arg" ]]; then
            # Use --as snapshot name
            if [[ "$as_arg" =~ ^@ ]]; then
                local snapshot_name="${as_arg#@}"
                
                # Handle special snapshot names
                case "$snapshot_name" in
                    today)
                        BACKUP_SNAPSHOT="${SNAP_PREFIX}-$(date +%Y%m%d)"
                        ;;
                    now)
                        BACKUP_SNAPSHOT="${SNAP_PREFIX}-$(date +%Y%m%d-%H%M%S)"
                        ;;
                    *)
                        BACKUP_SNAPSHOT="$snapshot_name"
                        ;;
                esac
            else
                echo "ERROR: --as for backup must start with @ (e.g., --as @today)"
                exit 1
            fi
        else
            # Default snapshot name
            BACKUP_SNAPSHOT="${SNAP_PREFIX}-$(date +%Y%m%d-%H%M%S)"
        fi
        
        echo "Creating new snapshot: ${BACKUP_DATASET}@${BACKUP_SNAPSHOT}"
        if ! zfs snapshot "${BACKUP_DATASET}@${BACKUP_SNAPSHOT}"; then
            echo "ERROR: Failed to create snapshot"
            exit 1
        fi
        
        # Wait for snapshot to be fully available
        echo "Waiting for snapshot to be fully available..."
        local retry_count=0
        while [[ $retry_count -lt 30 ]]; do
            if zfs list "${BACKUP_DATASET}@${BACKUP_SNAPSHOT}" >/dev/null 2>&1; then
                # Double-check snapshot is ready by trying to get its properties
                if zfs get -H -o value creation "${BACKUP_DATASET}@${BACKUP_SNAPSHOT}" >/dev/null 2>&1; then
                    echo "✓ Snapshot ready for backup"
                    break
                fi
            fi
            echo "  Waiting for snapshot to be ready... (${retry_count}/30)"
            sleep 1
            ((retry_count++))
        done
        
        if [[ $retry_count -eq 30 ]]; then
            echo "ERROR: Snapshot creation timed out or snapshot not accessible"
            exit 1
        fi
    fi
    
    # Validate dataset exists
    if ! zfs list "$BACKUP_DATASET" >/dev/null 2>&1; then
        echo "ERROR: Dataset '$BACKUP_DATASET' not found"
        exit 1
    fi
    
    # Validate age key
    if [[ ! -f "$AGE_PUBLIC_KEY_FILE" ]]; then
        echo "ERROR: Age public key not found at $AGE_PUBLIC_KEY_FILE"
        exit 1
    fi
    
    # Provider selection
    if [[ "$provider" == "all" ]]; then
        SELECTED_PROVIDERS=($(printf '%s\n' "${!PROVIDERS[@]}" | sort))
        echo "Selected providers: ${SELECTED_PROVIDERS[*]}"
    else
        # Handle legacy aliases
        case "$provider" in
            bb) provider="backblaze" ;;
            scw) provider="scaleway" ;;
        esac
        
        if [[ -z "${PROVIDERS[$provider]:-}" ]]; then
            echo "ERROR: Unknown provider '$provider'"
            echo "Available providers: ${!PROVIDERS[*]}"
            exit 1
        fi
        
        SELECTED_PROVIDERS=("$provider")
    fi
    
    # Test connections
    echo "Testing connections to selected providers..."
    for prov in "${SELECTED_PROVIDERS[@]}"; do
        remote="${PROVIDERS[$prov]}"
        echo "  Testing $prov (${PROVIDER_NAMES[$prov]})..."
        if ! rclone lsd "${remote%:*}:" >/dev/null 2>&1; then
            echo "ERROR: Cannot connect to $prov remote ${remote%:*}"
            exit 1
        fi
        echo "  ✓ Connected to $prov"
    done
    
    # Setup logging
    DATASET_LOG_FILE="$HOME/offsite-backup-$(echo "$BACKUP_DATASET" | tr '/' '_')-$(date +%Y%m%d-%H%M%S).log"
    # Simple logging without process substitution
    touch "$DATASET_LOG_FILE"
    
    # Create lockfile to prevent concurrent backups of same dataset
    LOCK_FILE="$HOME/.offsite-$(echo "$BACKUP_DATASET" | tr '/' '_').lock"
    
    # Check for existing lock
    if [[ -f "$LOCK_FILE" ]]; then
        local existing_pid=$(cat "$LOCK_FILE" 2>/dev/null || echo "")
        
        if [[ -z "$existing_pid" ]]; then
            echo "INFO: Removing corrupted lock file (no PID found)"
            rm -f "$LOCK_FILE"
        elif kill -0 "$existing_pid" 2>/dev/null; then
            # Process is still running - check if it's actually offsite
            local process_cmd=$(ps -p "$existing_pid" -o comm= 2>/dev/null || echo "")
            if [[ "$process_cmd" == *"offsite"* ]] || [[ "$process_cmd" == *"bash"* ]]; then
                echo "ERROR: Another offsite backup is already running for dataset $BACKUP_DATASET (PID $existing_pid)"
                echo "Process: $process_cmd"
                echo "If you're sure no other backup is running, remove: $LOCK_FILE"
                exit 1
            else
                echo "INFO: Removing stale lock file (PID $existing_pid is not offsite process: $process_cmd)"
                rm -f "$LOCK_FILE"
            fi
        else
            # PID doesn't exist - could be from reboot or killed process
            local lock_age=$(stat -c %Y "$LOCK_FILE" 2>/dev/null || echo "0")
            local current_time=$(date +%s)
            local age_minutes=$(( (current_time - lock_age) / 60 ))
            
            if [[ $age_minutes -gt 60 ]]; then
                echo "INFO: Removing old stale lock file (PID $existing_pid, ${age_minutes} minutes old - likely from reboot)"
            else
                echo "INFO: Removing stale lock file (PID $existing_pid no longer exists)"
            fi
            rm -f "$LOCK_FILE"
        fi
    fi
    
    # Create lock file with our PID
    echo $$ > "$LOCK_FILE"
    
    # Setup cleanup trap to remove lock file
    cleanup_lock() {
        rm -f "$LOCK_FILE"
    }
    trap cleanup_lock EXIT
    
    # Background-safe logging function
    log_message() {
        local message="$1"
        echo "$message" | tee -a "$DATASET_LOG_FILE"
    }
    
    # Check if running in background and inform user
    if [[ ! -t 1 ]]; then
        log_message "$(date '+%Y-%m-%d %H:%M:%S') INFO: Running in background mode"
        log_message "$(date '+%Y-%m-%d %H:%M:%S') INFO: Progress logged to $DATASET_LOG_FILE"
    fi
    
    echo "$(date '+%Y-%m-%d %H:%M:%S') === Offsite Backup Started ==="
    echo "$(date '+%Y-%m-%d %H:%M:%S') Dataset: $BACKUP_DATASET"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Snapshot: $BACKUP_SNAPSHOT"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Providers: ${SELECTED_PROVIDERS[*]}"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Force full: $force_full"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Log file: $DATASET_LOG_FILE"
    
    # Backup to each provider
    BACKUP_START=$(date +%s)
    
    for prov in "${SELECTED_PROVIDERS[@]}"; do
        rclone_remote="${PROVIDERS[$prov]}"
        provider_name="${PROVIDER_NAMES[$prov]}"
        backup_single_provider "$BACKUP_DATASET" "$BACKUP_SNAPSHOT" "$prov" "$rclone_remote" "$provider_name" "$force_full"
    done
    
    # Cleanup old snapshots
    cleanup_old_snapshots "$BACKUP_DATASET"
    
    BACKUP_END=$(date +%s)
    TOTAL_TIME=$((BACKUP_END - BACKUP_START))
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')"
    echo "$(date '+%Y-%m-%d %H:%M:%S') === Offsite Backup Completed ==="
    echo "$(date '+%Y-%m-%d %H:%M:%S') Dataset: $BACKUP_DATASET"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Snapshot: $BACKUP_SNAPSHOT"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Providers: ${SELECTED_PROVIDERS[*]}"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Total time: ${TOTAL_TIME}s ($(($TOTAL_TIME / 60))m $(($TOTAL_TIME % 60))s)"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Log saved to: $DATASET_LOG_FILE"
}

backup_single_provider() {
    local dataset="$1"
    local snap="$2"
    local provider="$3"
    local rclone_remote="$4"
    local provider_name="$5"
    local force_full="$6"
    local dataset_path="${dataset//\//_}"
    
    echo "$(date '+%Y-%m-%d %H:%M:%S') [+] Backing up ${dataset}@${snap} to $provider_name"
    
    # Detect previous snapshot for incremental backup
    local prev_snap=""
    if [[ "$force_full" == "false" ]]; then
        prev_snap=$(zfs list -H -t snapshot -o name -s creation | grep "^${dataset}@${SNAP_PREFIX}-" | grep -v "@${snap}$" | tail -n1 | awk -F@ '{print $2}' || true)
    fi
    
    # No working directory needed for streaming backup!
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Using streaming backup - no temp files required"
    
    # Get trimmed age public key
    local agekey=$(cat "$AGE_PUBLIC_KEY_FILE" | tr -d '\n')
    
    if [[ -n "$prev_snap" ]]; then
        echo "$(date '+%Y-%m-%d %H:%M:%S')     Incremental backup from $prev_snap"
        local prefix="incr"
        local backup_prefix="${prefix}-${snap}"
        
        stream_zfs_to_cloud "${dataset}@${prev_snap}" "${dataset}@${snap}" "$backup_prefix" "$rclone_remote" "$dataset_path" "$agekey" "incremental"
    else
        echo "$(date '+%Y-%m-%d %H:%M:%S')     Full backup"
        local prefix="full"
        local backup_prefix="${prefix}-${snap}"
        
        stream_zfs_to_cloud "" "${dataset}@${snap}" "$backup_prefix" "$rclone_remote" "$dataset_path" "$agekey" "full"
    fi
    
    # No cleanup needed - streaming backup used no temp files!
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')     ✓ Backup complete to $provider_name"
}

# Streaming ZFS backup function - no temp files needed!
stream_zfs_to_cloud() {
    local prev_snap="$1"
    local current_snap="$2"
    local backup_prefix="$3"
    local rclone_remote="$4"
    local dataset_path="$5"
    local agekey="$6"
    local backup_type="$7"
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Starting streaming backup ($backup_type)"
    
    # Check for existing shards (resumable backup support)
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Checking for existing shards..."
    local existing_shards=($(rclone lsf "${rclone_remote}/${dataset_path}/" 2>/dev/null | grep "^${backup_prefix}-s[0-9][0-9][0-9].zfs.gz.age$" | sort || true))
    
    if [[ ${#existing_shards[@]} -gt 0 ]]; then
        echo "$(date '+%Y-%m-%d %H:%M:%S')     Found ${#existing_shards[@]} existing shards - resuming backup"
        echo "$(date '+%Y-%m-%d %H:%M:%S')       Existing: ${existing_shards[0]} ... ${existing_shards[-1]}"
    else
        echo "$(date '+%Y-%m-%d %H:%M:%S')     No existing shards found - starting fresh backup"
    fi
    
    # Create the zfs send command
    local zfs_cmd
    if [[ "$backup_type" == "incremental" && -n "$prev_snap" ]]; then
        zfs_cmd="zfs send -I \"$prev_snap\" \"$current_snap\""
    else
        zfs_cmd="zfs send \"$current_snap\""
    fi
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Streaming: $zfs_cmd"
    
    # Estimate number of shards based on dataset size
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Estimating backup size..."
    local dataset_size_bytes=0
    local estimated_shards=0
    
    # Get dataset size for progress estimation
    if [[ "$backup_type" == "incremental" && -n "$prev_snap" ]]; then
        # For incremental: estimate based on changes since last snapshot
        # This is approximate - actual size depends on data changes
        dataset_size_bytes=$(zfs get -H -o value -p written "$current_snap" 2>/dev/null | grep -v '^-$' || echo "0")
        if [[ "$dataset_size_bytes" == "0" || "$dataset_size_bytes" == "-" ]]; then
            # Fallback: estimate as 10% of full dataset for incremental
            local full_size=$(zfs get -H -o value -p used "${current_snap%@*}" 2>/dev/null || echo "1073741824")
            dataset_size_bytes=$((full_size / 10))
        fi
    else
        # For full backup: get dataset used size
        dataset_size_bytes=$(zfs get -H -o value -p used "${current_snap%@*}" 2>/dev/null || echo "1073741824")
    fi
    
    # Calculate estimated shards based on SHARD_SIZE
    # Convert SHARD_SIZE to bytes (support K, M, G suffixes)
    local shard_size_bytes
    case "$SHARD_SIZE" in
        *K) shard_size_bytes=$((${SHARD_SIZE%K} * 1024)) ;;
        *M) shard_size_bytes=$((${SHARD_SIZE%M} * 1024 * 1024)) ;;
        *G) shard_size_bytes=$((${SHARD_SIZE%G} * 1024 * 1024 * 1024)) ;;
        *) shard_size_bytes="$SHARD_SIZE" ;;  # Assume bytes if no suffix
    esac
    estimated_shards=$(((dataset_size_bytes + shard_size_bytes - 1) / shard_size_bytes))  # Round up
    estimated_shards=$((estimated_shards > 0 ? estimated_shards : 1))  # Minimum 1 shard
    
    # Show size estimation
    local size_mb=$((dataset_size_bytes / 1024 / 1024))
    if [[ $size_mb -gt 1024 ]]; then
        local size_gb=$((size_mb / 1024))
        echo "$(date '+%Y-%m-%d %H:%M:%S')     Estimated size: ${size_gb}GB (~${estimated_shards} shards)"
    else
        echo "$(date '+%Y-%m-%d %H:%M:%S')     Estimated size: ${size_mb}MB (~${estimated_shards} shards)"
    fi
    
    # Stream with minimal temp space - use named pipes for zero-copy streaming
    local shard_num=1
    local total_shards=0
    
    # Create a temporary directory for named pipes only
    local pipe_dir=$(mktemp -d)
    local shard_pipe="$pipe_dir/shard_pipe"
    
    # Create named pipe for streaming
    mkfifo "$shard_pipe"
    
    # Background process to handle sharding
    {
        eval "$zfs_cmd" | split -b "$SHARD_SIZE" --numeric-suffixes=1 --suffix-length=3 - "$pipe_dir/shard-"
        echo "SPLIT_DONE" > "$shard_pipe"
    } &
    local split_pid=$!
    
    # Process shards as they appear - wait for complete shards
    local processed_shards=()
    while true; do
        # Get list of shard files
        local current_shards=($(ls "$pipe_dir"/shard-* 2>/dev/null | sort))
        
        # Process any new complete shards
        for shard_file in "${current_shards[@]}"; do
            [[ -f "$chunk_file" ]] || continue
            
            # Skip if already processed
            local shard_basename=$(basename "$shard_file")
            if printf '%s\n' "${processed_shards[@]}" | grep -q "^${shard_basename}$"; then
                continue
            fi
            
            # Wait for shard to be completely written by checking if it's still growing
            local initial_size=$(stat -c%s "$shard_file" 2>/dev/null || echo "0")
            sleep 0.1
            local final_size=$(stat -c%s "$shard_file" 2>/dev/null || echo "0")
            
            # If file is still growing, skip for now
            if [[ $initial_size -ne $final_size ]]; then
                continue
            fi
            
            # If split is still running and this isn't the last shard, wait a bit more
            if kill -0 $split_pid 2>/dev/null; then
                # Check if there might be more shards coming
                local next_shard_num=$(printf "%03d" $((10#${shard_basename#shard-} + 1)))
                if [[ ! -f "$pipe_dir/shard-$next_shard_num" ]]; then
                    # Wait a bit more to ensure shard is complete
                    sleep 0.2
                    local verify_size=$(stat -c%s "$shard_file" 2>/dev/null || echo "0")
                    if [[ $final_size -ne $verify_size ]]; then
                        continue  # Still growing
                    fi
                fi
            fi
            
            # Now process the complete shard
            local output_file="${backup_prefix}-s${shard_basename#shard-}.zfs.gz.age"
            
            # Check if shard already exists (resumable backup)
            if printf '%s\n' "${existing_shards[@]}" | grep -q "^${output_file}$"; then
                echo "$(date '+%Y-%m-%d %H:%M:%S')       ✓ Shard $shard_num of ~$estimated_shards already exists: $output_file"
            else
                echo "$(date '+%Y-%m-%d %H:%M:%S')       Processing shard $shard_num of ~$estimated_shards..."
                
                # Stream shard: compress -> encrypt -> upload
                if gzip < "$shard_file" | age -r "$agekey" | rclone rcat "${rclone_remote}/${dataset_path}/${output_file}"; then
                    echo "$(date '+%Y-%m-%d %H:%M:%S')         ✓ Uploaded $output_file"
                else
                    echo "$(date '+%Y-%m-%d %H:%M:%S')         ✗ Failed to upload $output_file"
                    kill $split_pid 2>/dev/null || true
                    rm -rf "$pipe_dir"
                    return 1
                fi
            fi
            
            # Mark as processed and remove file
            processed_shards+=("$shard_basename")
            rm -f "$shard_file"
            ((shard_num++))
            ((total_shards++))
        done
        
        # Check if split is done
        if ! kill -0 $split_pid 2>/dev/null; then
            # Process any remaining shards
            for shard_file in "$pipe_dir"/shard-*; do
                [[ -f "$shard_file" ]] || continue
                
                local shard_basename=$(basename "$shard_file")
                if printf '%s\n' "${processed_shards[@]}" | grep -q "^${shard_basename}$"; then
                    continue
                fi
                
                local output_file="${backup_prefix}-s${shard_basename#shard-}.zfs.gz.age"
                
                if ! printf '%s\n' "${existing_shards[@]}" | grep -q "^${output_file}$"; then
                    echo "$(date '+%Y-%m-%d %H:%M:%S')       Processing final shard $shard_num of ~$estimated_shards..."
                    gzip < "$shard_file" | age -r "$agekey" | rclone rcat "${rclone_remote}/${dataset_path}/${output_file}"
                    ((total_shards++))
                fi
                rm -f "$shard_file"
            done
            break
        fi
        
        # Small delay before checking again
        sleep 0.2
    done
    
    # Cleanup
    rm -rf "$pipe_dir"
    
    # Show completion with actual vs estimated
    if [[ $total_shards -eq $estimated_shards ]]; then
        echo "$(date '+%Y-%m-%d %H:%M:%S')     ✓ Streaming complete: $total_shards shards processed (as estimated)"
    else
        echo "$(date '+%Y-%m-%d %H:%M:%S')     ✓ Streaming complete: $total_shards shards processed (estimated $estimated_shards)"
    fi
}

cleanup_old_snapshots() {
    local dataset="$1"
    echo "$(date '+%Y-%m-%d %H:%M:%S') [+] Cleaning up old snapshots for $dataset"
    
    zfs list -H -t snapshot -o name,creation -s creation | grep "^${dataset}@${SNAP_PREFIX}-" | while read snap creation; do
        local snap_time=$(echo "$snap" | awk -F@ '{print $2}' | sed 's/.*-//')
        local snap_date=$(date -d "${snap_time:0:8} ${snap_time:8:2}:${snap_time:10:2}:${snap_time:12:2}" +%s 2>/dev/null || continue)
        local now=$(date +%s)
        local age_days=$(( (now - snap_date) / 86400 ))
        
        if (( age_days > RETENTION_DAYS )); then
            echo "$(date '+%Y-%m-%d %H:%M:%S')     Destroying old snapshot $snap (${age_days} days old)"
            zfs destroy "$snap"
        fi
    done
}

# === END BACKUP FUNCTIONS ===

# === RESTORE FUNCTIONS ===

run_restore() {
    local dataset_arg="$1"
    local provider="${2:-auto}"
    local as_arg="$3"
    
    # Parse dataset@backup syntax
    if [[ "$dataset_arg" =~ @ ]]; then
        local source_dataset="${dataset_arg%@*}"
        local backup_prefix="${dataset_arg#*@}"
        
        # ZFS semantics: dataset and dataset@latest are identical
        if [[ "$backup_prefix" == "latest" ]]; then
            echo "Note: dataset@latest is the same as dataset (full dataset state)"
        fi
    else
        local source_dataset="$dataset_arg"
        local backup_prefix="latest"
    fi
    
    # Set target dataset from --as or use original name
    local target_dataset
    if [[ -n "$as_arg" ]]; then
        target_dataset="$as_arg"
    else
        target_dataset="$source_dataset"
    fi
    
    # Validate age key
    if [[ ! -f "$AGE_PRIVATE_KEY_FILE" ]]; then
        echo "ERROR: Age private key not found at $AGE_PRIVATE_KEY_FILE"
        exit 1
    fi
    
    # Provider selection
    case "$provider" in
        auto)
            # Try all providers, prefer backblaze
            echo "Auto mode: Testing all providers..."
            local dataset_path="${source_dataset//\//_}"
            local selected_remote=""
            local provider_name=""
            
            # Try providers in order of preference
            for prov in backblaze scaleway $(printf '%s\n' "${!PROVIDERS[@]}" | grep -v -E '^(backblaze|scaleway)$' | sort); do
                local remote="${PROVIDERS[$prov]}"
                if rclone lsf "${remote}/${dataset_path}/" 2>/dev/null | grep -q "^.*-x001.zfs.gz.age$"; then
                    selected_remote="$remote"
                    provider_name="${PROVIDER_NAMES[$prov]} - auto-selected"
                    echo "Found backups on $prov"
                    break
                fi
            done
            
            if [[ -z "$selected_remote" ]]; then
                echo "ERROR: No backups found on any provider"
                echo "Available providers: ${!PROVIDERS[*]}"
                exit 1
            fi
            ;;
        *)
            # Handle legacy aliases
            case "$provider" in
                bb) provider="backblaze" ;;
                scw) provider="scaleway" ;;
            esac
            
            if [[ -z "${PROVIDERS[$provider]:-}" ]]; then
                echo "ERROR: Unknown provider '$provider'"
                echo "Available providers: ${!PROVIDERS[*]}"
                exit 1
            fi
            
            selected_remote="${PROVIDERS[$provider]}"
            provider_name="${PROVIDER_NAMES[$provider]}"
            ;;
    esac
    
    # Setup variables
    local dataset_path="${source_dataset//\//_}"
    local remote_path="${selected_remote}/${dataset_path}"
    
    echo "=== Offsite Restore ==="
    echo "Provider: $provider_name"
    echo "Source dataset: $source_dataset"
    echo "Target dataset: $target_dataset"
    echo "Remote path: $remote_path"
    echo ""
    
    # Backup selection
    if [[ "$backup_prefix" == "latest" ]] || [[ "$backup_prefix" == "latest-full" ]]; then
        echo "Discovering available backups..."
        
        # Get all backup prefixes for this dataset
        local all_backups=($(rclone lsf "${remote_path}/" 2>/dev/null | grep -E '^(full|incr)-.*-x[0-9][0-9][0-9]\.zfs\.gz\.age$' | sed 's/-x[0-9][0-9][0-9]\.zfs\.gz\.age$//' | sort -u | sort -t- -k3,3 || true))
        
        if [[ ${#all_backups[@]} -eq 0 ]]; then
            echo "ERROR: No backups found for dataset $source_dataset"
            exit 1
        fi
        
        if [[ "$backup_prefix" == "latest-full" ]]; then
            # Find latest full backup
            local selected_backup=""
            for backup in "${all_backups[@]}"; do
                if [[ "$backup" =~ ^full- ]]; then
                    selected_backup="$backup"
                fi
            done
            
            if [[ -z "$selected_backup" ]]; then
                echo "ERROR: No full backups found for dataset $source_dataset"
                exit 1
            fi
            
            backup_prefix="$selected_backup"
            echo "Selected latest full backup: $backup_prefix"
        else
            # Find latest backup (incremental preferred)
            backup_prefix="${all_backups[-1]}"
            echo "Selected latest backup: $backup_prefix"
        fi
        
        echo "Available backups for $source_dataset:"
        printf '%s\n' "${all_backups[@]}" | tail -5
        echo ""
    else
        # User specified a specific backup - need to find the actual backup prefix
        echo "Searching for backup matching: $backup_prefix"
        
        # Get all backup prefixes for this dataset
        local all_backups=($(rclone lsf "${remote_path}/" 2>/dev/null | grep -E '^(full|incr)-.*-x[0-9][0-9][0-9]\.zfs\.gz\.age$' | sed 's/-x[0-9][0-9][0-9]\.zfs\.gz\.age$//' | sort -u | sort -t- -k3,3 || true))
        
        if [[ ${#all_backups[@]} -eq 0 ]]; then
            echo "ERROR: No backups found for dataset $source_dataset"
            exit 1
        fi
        
        # Look for backup that ends with the specified snapshot name
        local found_backup=""
        for backup in "${all_backups[@]}"; do
            if [[ "$backup" == "full-${backup_prefix}" ]] || [[ "$backup" == "incr-${backup_prefix}" ]]; then
                found_backup="$backup"
                break
            fi
        done
        
        if [[ -z "$found_backup" ]]; then
            echo "ERROR: No backup found matching snapshot: $backup_prefix"
            echo "Available backups:"
            printf '%s\n' "${all_backups[@]}"
            exit 1
        fi
        
        backup_prefix="$found_backup"
        echo "Found matching backup: $backup_prefix"
    fi
    
    echo "Final backup selection: $backup_prefix"
    echo ""
    
    # Discover shards
    echo "Discovering backup shards..."
    local shard_files=($(rclone lsf "${remote_path}/" 2>/dev/null | grep "^${backup_prefix}-s[0-9][0-9][0-9].zfs.gz.age$" | sort))
    
    if [[ ${#shard_files[@]} -eq 0 ]]; then
        echo "ERROR: No shards found for backup prefix: ${backup_prefix}"
        echo ""
        echo "Available backups for ${source_dataset}:"
        rclone lsf "${remote_path}/" 2>/dev/null | grep -E '^(full|incr)-.*-x[0-9][0-9][0-9]\.zfs\.gz\.age$' | sed 's/-x[0-9][0-9][0-9]\.zfs\.gz\.age$//' | sort -u || echo "No backups found"
        exit 1
    fi
    
    echo "Found ${#shard_files[@]} shards: ${shard_files[0]} ... ${shard_files[-1]}"
    
    # Connection test
    echo "Testing connection to $provider_name..."
    if ! rclone lsd "${selected_remote%:*}:" >/dev/null 2>&1; then
        echo "ERROR: Cannot connect to $provider_name"
        exit 1
    fi
    echo "✓ Connected to $provider_name"
    
    # Target dataset validation
    if zfs list "$target_dataset" >/dev/null 2>&1; then
        # Check if it's a pool or dataset
        local target_type=$(zfs get -H -o value type "$target_dataset" 2>/dev/null)
        
        if [[ "$target_type" == "filesystem" ]]; then
            echo "WARNING: Target dataset $target_dataset already exists"
            
            # Check if running in background (no TTY)
            if [[ ! -t 0 ]]; then
                echo "ERROR: Cannot prompt for confirmation when running in background"
                echo "Target dataset already exists. Use --as to specify a different target name"
                exit 1
            fi
            
            read -p "Do you want to destroy and recreate it? (y/N): " -n 1 -r
            echo
            if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                echo "Restore cancelled"
                echo "Suggestion: Use --as to specify a different target name"
                exit 1
            fi
            echo "Destroying existing dataset..."
            zfs destroy -r "$target_dataset"
        else
            echo "ERROR: Target $target_dataset exists but is not a filesystem"
            echo "Type: $target_type"
            echo "Use --as to specify a different target name"
            exit 1
        fi
    else
        # Check if parent exists for dataset creation
        local parent_dataset=$(dirname "$target_dataset" | sed 's|/.*||' | head -1)
        local dataset_path=$(echo "$target_dataset" | sed 's|^[^/]*/||')
        
        # If target contains '/', check parent exists
        if [[ "$target_dataset" =~ / ]]; then
            local parent_path=$(echo "$target_dataset" | sed 's|/[^/]*$||')
            if ! zfs list "$parent_path" >/dev/null 2>&1; then
                echo "ERROR: Parent dataset $parent_path does not exist"
                echo "Create parent first or use --as with an existing parent"
                exit 1
            fi
        fi
    fi
    
    # Restore process
    local work_dir="${TEMP_DIR}/offsite-restore-$(date +%s)"
    mkdir -p "$work_dir"
    cd "$work_dir"
    
    echo ""
    echo "Starting restore process..."
    echo "Working directory: $work_dir"
    
    # Download all shards
    echo "Step 1: Downloading shards..."
    for shard_file in "${shard_files[@]}"; do
        echo "  Downloading: $shard_file"
        rclone copy "${remote_path}/${shard_file}" ./
    done
    
    # Decrypt and decompress shards
    echo "Step 2: Decrypting and decompressing shards..."
    for shard_file in "${shard_files[@]}"; do
        echo "  Processing: $shard_file"
        # Extract the shard number from filename
        local shard_num=$(echo "$shard_file" | sed 's/.*-s\([0-9][0-9][0-9]\)\.zfs\.gz\.age$/\1/')
        age -d -i "$AGE_PRIVATE_KEY_FILE" "$shard_file" | gunzip > "shard-${shard_num}.restored"
        echo "    ✓ Restored: shard-${shard_num}.restored"
    done
    
    # Reconstitute stream
    echo "Step 3: Reconstituting ZFS stream..."
    cat shard-*.restored > complete-stream.zfs
    echo "Complete stream size: $(wc -c < complete-stream.zfs) bytes"
    
    # ZFS receive
    echo "Step 4: Performing ZFS receive..."
    zfs recv -F "$target_dataset" < complete-stream.zfs
    
    # Cleanup
    cd /
    rm -rf "$work_dir"
    
    echo ""
    echo "✅ Restore complete!"
    echo ""
    echo "Source: $source_dataset ($backup_prefix)"
    echo "Target: $target_dataset"
    echo "Shards processed: ${#shard_files[@]}"
    echo "Provider: $provider_name"
    echo ""
    echo "You can now:"
    echo "  - Mount: zfs mount $target_dataset"
    echo "  - List snapshots: zfs list -t snapshot -r $target_dataset"
    echo "  - Access data at: $(zfs get -H -o value mountpoint $target_dataset 2>/dev/null || echo 'mount point')"
}

# === END RESTORE FUNCTIONS ===