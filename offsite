#!/bin/bash
set -euo pipefail

# Offsite - Unified ZFS Cloud Backup Tool v2.0
# Handles both backup and restore operations with ZFS-style syntax
# Features: Configurable shard sizes, streaming architecture, multi-terminal safety

# === USAGE ===
show_help() {
    cat << 'EOF'
Usage: offsite [--backup|--restore] <dataset[@backup]> [options]

OPERATIONS:
  --backup, -b    Backup a ZFS dataset to cloud storage
  --restore, -r   Restore a ZFS dataset from cloud storage
  --list, -l      List available backups and show completion status

ZFS-STYLE SYNTAX:
  dataset               - For backup: create auto-timestamped snapshot and backup
                         For restore: restore full dataset state (latest backup)
  dataset@snapshot      - For backup: create/use specific snapshot and backup
                         For restore: restore to specific backup point
  dataset@now           - For backup: create snapshot literally named 'now'
  dataset@today         - For backup: create snapshot with today's date
  dataset@latest        - For restore: same as dataset (ZFS semantics)
  dataset@latest-full   - For restore: most recent full backup only

BACKUP OPTIONS:
  --provider <name>     Provider to backup to (default: all)

RESTORE OPTIONS:
  --as <dataset>        Target dataset name (default: restore to original name)
  --provider <name>     Provider to restore from (default: auto-detect)

COMMON OPTIONS:
  --help, -h           Show this help
  --version, -v        Show version information

EXAMPLES:
  # Backup operations (always full backups)
  offsite --backup tank/important/data                    # Auto-timestamped snapshot
  offsite --backup tank/photos@now --provider scaleway    # Snapshot named 'now'
  offsite --backup tank/docs@backup-$(date +%Y%m%d)       # Custom snapshot name
  offsite --backup tank/projects@today                    # Snapshot with date
  
  # Restore operations
  offsite --restore tank/important/data                 # Restore to original name
  offsite --restore tank/photos@latest-full --as tank/photos-restored
  offsite --restore tank/docs@full-auto-20250716-161605 --as tank/docs-test

  # Short form (auto-detects operation from context)
  offsite tank/important/data                    # Backup (default operation)
  offsite tank/important/data@latest             # Restore (@ implies restore)
  
NOTE: --as flag is only available for restore operations in version 2.0+

PROVIDERS:
  all         - Backup to all configured providers
  auto        - Auto-detect provider (restore only)
  backblaze   - Backblaze B2 (US)
  scaleway    - Scaleway (EU)
  <custom>    - Any configured provider (e.g., amazonS3)

CONFIGURATION:
  Config file: ~/.config/offsite/config.env
  Setup: run setup-offsite to configure

EOF
}

# === ARGUMENT PARSING ===
OPERATION=""
DATASET_ARG=""
PROVIDER=""
AS_ARG=""
VERSION="2.0"

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --backup|-b)
            OPERATION="backup"
            shift
            ;;
        --restore|-r)
            OPERATION="restore"
            shift
            ;;
        --list|-l)
            OPERATION="list"
            shift
            ;;
        --provider)
            PROVIDER="$2"
            shift 2
            ;;
        --as)
            # Only allow --as for restore operations
            if [[ "$OPERATION" == "restore" ]]; then
                AS_ARG="$2"
                shift 2
            else
                echo "ERROR: --as flag only valid for restore operations"
                exit 1
            fi
            ;;
        --help|-h)
            show_help
            exit 0
            ;;
        --version|-v)
            echo "offsite version $VERSION"
            exit 0
            ;;
        -*)
            echo "ERROR: Unknown option $1"
            show_help
            exit 1
            ;;
        *)
            if [[ -z "$DATASET_ARG" ]]; then
                DATASET_ARG="$1"
            else
                echo "ERROR: Multiple datasets specified"
                show_help
                exit 1
            fi
            shift
            ;;
    esac
done

# Validate required arguments
if [[ -z "$DATASET_ARG" ]]; then
    echo "ERROR: No dataset specified"
    show_help
    exit 1
fi

# === AUTO-DETECT OPERATION ===
if [[ -z "$OPERATION" ]]; then
    if [[ "$DATASET_ARG" =~ @ ]]; then
        OPERATION="restore"
        echo "Auto-detected: restore operation (dataset@backup syntax)"
    else
        OPERATION="backup"
        echo "Auto-detected: backup operation (default)"
    fi
fi

# === LOAD CONFIG ===
CONFIG_FILE="${OFFSITE_CONFIG:-$HOME/.config/offsite/config.env}"
if [[ -f "$CONFIG_FILE" ]]; then
    source "$CONFIG_FILE"
else
    echo "WARNING: Config file not found at $CONFIG_FILE"
    echo "Using default values or environment variables"
fi

# === CONFIG WITH DEFAULTS ===
AGE_PUBLIC_KEY_FILE="${AGE_PUBLIC_KEY_FILE:-$HOME/.config/age/zfs-backup.pub}"
AGE_PRIVATE_KEY_FILE="${AGE_PRIVATE_KEY_FILE:-$HOME/.config/age/zfs-backup.txt}"
SNAP_PREFIX="${ZFS_SNAP_PREFIX:-auto}"
RETENTION_DAYS="${ZFS_RETENTION_DAYS:-14}"
TEMP_DIR="${TEMP_DIR:-/tmp}"
SHARD_SIZE="${SHARD_SIZE:-1G}"
VERSION="2.0"

# === DISCOVER PROVIDERS ===
declare -A PROVIDERS
declare -A PROVIDER_NAMES

# Built-in providers
PROVIDERS[backblaze]="${RCLONE_REMOTE:-cloudremote:zfs-buckets-walnut-deepblack-cloud/zfs-backups}"
PROVIDER_NAMES[backblaze]="Backblaze (US)"

PROVIDERS[scaleway]="${SCALEWAY_REMOTE:-scaleway:zfs-buckets-acacia/zfs-backups}"
PROVIDER_NAMES[scaleway]="Scaleway (EU)"

# Discover custom providers from config (any variable ending with _REMOTE)
if [[ -f "$CONFIG_FILE" ]]; then
    while IFS='=' read -r key value; do
        [[ "$key" =~ ^[A-Z_]+_REMOTE$ ]] && [[ "$key" != "RCLONE_REMOTE" ]] && [[ "$key" != "SCALEWAY_REMOTE" ]] && {
            provider_name=$(echo "$key" | sed 's/_REMOTE$//' | tr '[:upper:]' '[:lower:]')
            PROVIDERS[$provider_name]="$value"
            PROVIDER_NAMES[$provider_name]="$provider_name"
        }
    done < <(grep -E '^[A-Z_]+_REMOTE=' "$CONFIG_FILE" | sed 's/^export //')
fi

# === VALIDATION ===
command -v zfs >/dev/null || { echo "ERROR: zfs not installed"; exit 1; }
command -v rclone >/dev/null || { echo "ERROR: rclone not installed"; exit 1; }
command -v age >/dev/null || { echo "ERROR: age not installed"; exit 1; }

# Check for optional tools and suggest installation
if ! command -v pv >/dev/null 2>&1; then
    echo "INFO: pv (pipe viewer) not found - install for upload progress bars"
    echo "      Ubuntu/Debian: sudo apt install pv"
    echo "      RHEL/CentOS: sudo yum install pv"
fi

# === DELEGATE TO OPERATION ===
case "$OPERATION" in
    backup)
        # Source the backup functionality
        source <(sed -n '/^# === BACKUP FUNCTIONS ===/,/^# === END BACKUP FUNCTIONS ===/p' "$0")
        run_backup "$DATASET_ARG" "$PROVIDER"
        ;;
    restore)
        # Source the restore functionality
        source <(sed -n '/^# === RESTORE FUNCTIONS ===/,/^# === END RESTORE FUNCTIONS ===/p' "$0")
        run_restore "$DATASET_ARG" "$PROVIDER" "$AS_ARG"
        ;;
    list)
        # Source the list functionality  
        source <(sed -n '/^# === LIST FUNCTIONS ===/,/^# === END LIST FUNCTIONS ===/p' "$0")
        run_list "$DATASET_ARG" "$PROVIDER"
        ;;
    *)
        echo "ERROR: Invalid operation: $OPERATION"
        show_help
        exit 1
        ;;
esac

exit 0

# === BACKUP FUNCTIONS ===

run_backup() {
    local dataset_arg="$1"
    local provider="${2:-all}"
    
    # Parse dataset@snapshot syntax or use --as
    if [[ "$dataset_arg" =~ @ ]]; then
        local dataset="${dataset_arg%@*}"
        local snapshot_spec="${dataset_arg#*@}"
        
        # First check if the snapshot exists as-is (don't transform existing snapshots)
        if zfs list "${dataset}@${snapshot_spec}" >/dev/null 2>&1; then
            # Snapshot exists - use it directly
            BACKUP_DATASET="$dataset"
            BACKUP_SNAPSHOT="$snapshot_spec"
            echo "Using existing snapshot: ${dataset}@${snapshot_spec}"
        else
            # Snapshot doesn't exist - handle special names and create if needed
            case "$snapshot_spec" in
                today)
                    snapshot_spec="${SNAP_PREFIX}-$(date +%Y%m%d)"
                    ;;
                # Note: @now creates a snapshot literally named 'now', not auto-timestamped
                # For auto-timestamped snapshots, use dataset without @
            esac
            
            # Check again after transformation
            if ! zfs list "${dataset}@${snapshot_spec}" >/dev/null 2>&1; then
                echo "Creating snapshot: ${dataset}@${snapshot_spec}"
                if ! zfs snapshot "${dataset}@${snapshot_spec}"; then
                    echo "ERROR: Failed to create snapshot"
                    exit 1
                fi
            fi
            
            BACKUP_DATASET="$dataset"
            BACKUP_SNAPSHOT="$snapshot_spec"
            echo "Backing up snapshot: ${dataset}@${snapshot_spec}"
        fi
    else
        # Create new snapshot
        BACKUP_DATASET="$dataset_arg"
        
        # Default snapshot name
        BACKUP_SNAPSHOT="${SNAP_PREFIX}-$(date +%Y%m%d-%H%M%S)"
        
        echo "Creating snapshot: ${BACKUP_DATASET}@${BACKUP_SNAPSHOT}"
        if ! zfs snapshot "${BACKUP_DATASET}@${BACKUP_SNAPSHOT}"; then
            echo "ERROR: Failed to create snapshot"
            exit 1
        fi
        
        # Wait for snapshot to be fully available
        echo "Waiting for snapshot to be fully available..."
        local retry_count=0
        while [[ $retry_count -lt 30 ]]; do
            if zfs list "${BACKUP_DATASET}@${BACKUP_SNAPSHOT}" >/dev/null 2>&1; then
                # Double-check snapshot is ready by trying to get its properties
                if zfs get -H -o value creation "${BACKUP_DATASET}@${BACKUP_SNAPSHOT}" >/dev/null 2>&1; then
                    echo "âœ“ Snapshot ready for backup"
                    break
                fi
            fi
            echo "  Waiting for snapshot to be ready... (${retry_count}/30)"
            sleep 1
            ((retry_count++))
        done
        
        if [[ $retry_count -eq 30 ]]; then
            echo "ERROR: Snapshot creation timed out or snapshot not accessible"
            exit 1
        fi
    fi
    
    # Validate dataset exists
    if ! zfs list "$BACKUP_DATASET" >/dev/null 2>&1; then
        echo "ERROR: Dataset '$BACKUP_DATASET' not found"
        exit 1
    fi
    
    # Validate age key
    if [[ ! -f "$AGE_PUBLIC_KEY_FILE" ]]; then
        echo "ERROR: Age public key not found at $AGE_PUBLIC_KEY_FILE"
        exit 1
    fi
    
    # Provider selection
    if [[ "$provider" == "all" ]]; then
        SELECTED_PROVIDERS=($(printf '%s\n' "${!PROVIDERS[@]}" | sort))
        echo "Selected providers: ${SELECTED_PROVIDERS[*]}"
    else
        # Handle legacy aliases
        case "$provider" in
            bb) provider="backblaze" ;;
            scw) provider="scaleway" ;;
        esac
        
        if [[ -z "${PROVIDERS[$provider]:-}" ]]; then
            echo "ERROR: Unknown provider '$provider'"
            echo "Available providers: ${!PROVIDERS[*]}"
            exit 1
        fi
        
        SELECTED_PROVIDERS=("$provider")
    fi
    
    # Test connections
    echo "Testing connections to selected providers..."
    for prov in "${SELECTED_PROVIDERS[@]}"; do
        remote="${PROVIDERS[$prov]}"
        echo "  Testing $prov (${PROVIDER_NAMES[$prov]})..."
        if ! rclone lsd "${remote%:*}:" >/dev/null 2>&1; then
            echo "ERROR: Cannot connect to $prov remote ${remote%:*}"
            exit 1
        fi
        echo "  âœ“ Connected to $prov"
    done
    
    # Setup logging
    DATASET_LOG_FILE="$HOME/offsite-backup-$(echo "$BACKUP_DATASET" | tr '/' '_')-$(date +%Y%m%d-%H%M%S).log"
    # Simple logging without process substitution
    touch "$DATASET_LOG_FILE"
    
    # Create lockfile to prevent concurrent backups of same dataset
    LOCK_FILE="$HOME/.offsite-$(echo "$BACKUP_DATASET" | tr '/' '_').lock"
    
    # Check for existing lock
    if [[ -f "$LOCK_FILE" ]]; then
        local existing_pid=$(cat "$LOCK_FILE" 2>/dev/null || echo "")
        
        if [[ -z "$existing_pid" ]]; then
            echo "INFO: Removing corrupted lock file (no PID found)"
            rm -f "$LOCK_FILE"
        elif kill -0 "$existing_pid" 2>/dev/null; then
            # Process is still running - check if it's actually offsite
            local process_cmd=$(ps -p "$existing_pid" -o comm= 2>/dev/null || echo "")
            if [[ "$process_cmd" == *"offsite"* ]] || [[ "$process_cmd" == *"bash"* ]]; then
                echo "ERROR: Another offsite backup is already running for dataset $BACKUP_DATASET (PID $existing_pid)"
                echo "Process: $process_cmd"
                echo "If you're sure no other backup is running, remove: $LOCK_FILE"
                exit 1
            else
                echo "INFO: Removing stale lock file (PID $existing_pid is not offsite process: $process_cmd)"
                rm -f "$LOCK_FILE"
            fi
        else
            # PID doesn't exist - could be from reboot or killed process
            local lock_age=$(stat -c %Y "$LOCK_FILE" 2>/dev/null || echo "0")
            local current_time=$(date +%s)
            local age_minutes=$(( (current_time - lock_age) / 60 ))
            
            if [[ $age_minutes -gt 60 ]]; then
                echo "INFO: Removing old stale lock file (PID $existing_pid, ${age_minutes} minutes old - likely from reboot)"
            else
                echo "INFO: Removing stale lock file (PID $existing_pid no longer exists)"
            fi
            rm -f "$LOCK_FILE"
        fi
    fi
    
    # Create lock file with our PID
    echo $$ > "$LOCK_FILE"
    
    # Setup cleanup trap to remove lock file
    cleanup_lock() {
        rm -f "$LOCK_FILE"
    }
    trap cleanup_lock EXIT
    
    # Background-safe logging function
    log_message() {
        local message="$1"
        echo "$message" | tee -a "$DATASET_LOG_FILE"
    }
    
    # Check if running in background and inform user
    if [[ ! -t 1 ]]; then
        log_message "$(date '+%Y-%m-%d %H:%M:%S') INFO: Running in background mode"
        log_message "$(date '+%Y-%m-%d %H:%M:%S') INFO: Progress logged to $DATASET_LOG_FILE"
    fi
    
    echo "$(date '+%Y-%m-%d %H:%M:%S') === Offsite Backup Started ==="
    echo "$(date '+%Y-%m-%d %H:%M:%S') Dataset: $BACKUP_DATASET"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Snapshot: $BACKUP_SNAPSHOT"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Providers: ${SELECTED_PROVIDERS[*]}"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Backup type: full (always)"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Log file: $DATASET_LOG_FILE"
    
    # Backup to all providers in parallel (shard-by-shard)
    BACKUP_START=$(date +%s)
    
    backup_all_providers_parallel "$BACKUP_DATASET" "$BACKUP_SNAPSHOT"
    
    # Cleanup old snapshots
    cleanup_old_snapshots "$BACKUP_DATASET"
    
    BACKUP_END=$(date +%s)
    TOTAL_TIME=$((BACKUP_END - BACKUP_START))
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')"
    echo "$(date '+%Y-%m-%d %H:%M:%S') === Offsite Backup Completed ==="
    echo "$(date '+%Y-%m-%d %H:%M:%S') Dataset: $BACKUP_DATASET"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Snapshot: $BACKUP_SNAPSHOT"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Providers: ${SELECTED_PROVIDERS[*]}"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Total time: ${TOTAL_TIME}s ($(($TOTAL_TIME / 60))m $(($TOTAL_TIME % 60))s)"
    echo "$(date '+%Y-%m-%d %H:%M:%S') Log saved to: $DATASET_LOG_FILE"
}

backup_all_providers_parallel() {
    local dataset="$1"
    local snap="$2"
    local dataset_path="${dataset//\//_}"
    
        if [[ ${#SELECTED_PROVIDERS[@]} -gt 1 ]]; then
        echo "$(date '+%Y-%m-%d %H:%M:%S') [+] Starting parallel backup to ${#SELECTED_PROVIDERS[@]} providers: ${SELECTED_PROVIDERS[*]}"
    else
        echo "$(date '+%Y-%m-%d %H:%M:%S') [+] Starting backup to provider: ${SELECTED_PROVIDERS[0]}"
    fi
    
    # Always perform full backups - simpler and more reliable
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Performing full backup"
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Using streaming backup - no temp files required"
    
    # Get trimmed age public key
    local agekey=$(cat "$AGE_PUBLIC_KEY_FILE" | tr -d '\n')
    
    # Create backup prefix with full dataset@snapshot info: rust/test@now -> rust_test:now
    local dataset_clean="${dataset//\//_}"  # rust/test -> rust_test
    local backup_prefix="${dataset_clean}:${snap}"  # rust_test:now
    
    stream_zfs_to_cloud "${dataset}@${snap}" "$backup_prefix" "$dataset_path" "$agekey"
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')     âœ“ Parallel backup complete to all providers"
}

backup_single_provider() {
    local dataset="$1"
    local snap="$2"
    local provider="$3"
    local rclone_remote="$4"
    local provider_name="$5"
    local dataset_path="${dataset//\//_}"
    
    echo "$(date '+%Y-%m-%d %H:%M:%S') [+] Backing up ${dataset}@${snap} to $provider_name"
    
    # Always perform full backups - simpler and more reliable
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Performing full backup"
    
    # No working directory needed for streaming backup!
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Using streaming backup - no temp files required"
    
    # Get trimmed age public key
    local agekey=$(cat "$AGE_PUBLIC_KEY_FILE" | tr -d '\n')
    
    # Always do full backup
    echo "$(date '+%Y-%m-%d %H:%M:%S')     Full backup"
    
    # Create backup prefix with full dataset@snapshot info: rust/test@now -> rust_test:now
    local dataset_clean="${dataset//\//_}"  # rust/test -> rust_test
    local backup_prefix="${dataset_clean}:${snap}"  # rust_test:now
    
    stream_zfs_to_cloud "" "${dataset}@${snap}" "$backup_prefix" "$rclone_remote" "$dataset_path" "$agekey" "full"
    
    # No cleanup needed - streaming backup used no temp files!
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')     âœ“ Backup complete to $provider_name"
}

# Check if a backup is complete by comparing uploaded size to dataset size
is_backup_complete() {
    local rclone_remote="$1"
    local dataset_path="$2"
    local backup_prefix="$3"
    local original_dataset_size="$4"
    local shard_size="$5"

    # Get all shards for this backup with size information
    local shard_list_output=$(rclone lsf "${rclone_remote}/${dataset_path}/" --format "sp" 2>/dev/null | grep "^${backup_prefix}-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9].zfs.gz.age ")

    if [[ -z "$shard_list_output" ]]; then
        return 1  # No shards found
    fi

    local shard_sizes=()
    local total_uploaded_size=0
    while IFS= read -r line; do
        local size=$(echo "$line" | awk '{print $1}')
        shard_sizes+=("$size")
        total_uploaded_size=$((total_uploaded_size + size))
    done <<< "$shard_list_output"

    local shard_count=${#shard_sizes[@]}
    if [[ $shard_count -eq 0 ]]; then
        return 1 # No shards
    fi

    local last_shard_size=${shard_sizes[-1]}

    # Case 1: The last shard is smaller than the regular shard size.
    # This is the most reliable indicator of a complete backup.
    if [[ $last_shard_size -lt $shard_size ]]; then
        return 0 # COMPLETE: Final shard is smaller than a full shard.
    fi

    # Case 2: The last shard is full-sized. This could be an incomplete backup,
    # or a backup that ended exactly on a shard boundary.
    # We compare the total uploaded size to the original dataset size.
    # ZFS send streams are often smaller than `zfs get used`, so we use a threshold (e.g., 95%).
    local completion_percentage=95
    local required_size=$((original_dataset_size * completion_percentage / 100))

    if [[ $total_uploaded_size -ge $required_size ]]; then
        # The total size is large enough. We can be reasonably sure it's complete,
        # even if the last shard is full. This handles backups that end on a boundary.
        return 0 # COMPLETE: Total size is over 95% of original dataset size.
    fi

    # Case 3: If we are here, we have one or more full-sized shards, but the total size is
    # less than 95% of the dataset size. This is the classic "incomplete" case.
    return 1 # INCOMPLETE: Not enough data uploaded.
}





# New, simplified streaming function that streams the entire dataset from start to finish.
# It relies on the upload_shard_to_all_providers function to intelligently skip existing shards.
stream_zfs_to_cloud() {
    local current_snap="$1"
    local backup_prefix="$2"
    local dataset_path="$3"
    local agekey="$4"

    echo "$(date '+%Y-%m-%d %H:%M:%S')     Starting unified streaming backup..."

    # Get dataset size for shard calculation and progress bar
    local dataset_size_bytes=$(zfs get -H -o value -p used "${current_snap%@*}" 2>/dev/null || echo "1073741824")
    local dynamic_shard_size_bytes=$((dataset_size_bytes / 100))
    local min_shard_size=$((10 * 1024 * 1024))       # 10MB minimum
    local max_shard_size=$((1 * 1024 * 1024 * 1024))  # 1GB maximum

    if [[ $dynamic_shard_size_bytes -lt $min_shard_size ]]; then
        dynamic_shard_size_bytes=$min_shard_size
    elif [[ $dynamic_shard_size_bytes -gt $max_shard_size ]]; then
        dynamic_shard_size_bytes=$max_shard_size
    fi

    temp_dir=$(mktemp -d)
    trap 'rm -rf "$temp_dir"' EXIT

    echo "$(date '+%Y-%m-%d %H:%M:%S')     Streaming from ZFS snapshot..."

    # This pipeline streams the entire ZFS send and processes it shard by shard.
    zfs send "$current_snap" | pv -s "$dataset_size_bytes" | {
        local shard_num=1
        local current_byte_offset=0

        while true; do
            local shard_file="$temp_dir/shard-$(printf "%03d" "$shard_num")"
            
            # Read one shard's worth of data. head is more reliable than dd for this.
            bytes_read=$(head -c "$dynamic_shard_size_bytes" | tee "$shard_file" | wc -c)

            if [[ $bytes_read -eq 0 ]]; then
                rm -f "$shard_file"
                break # End of stream
            fi

            local output_file="${backup_prefix}-s$(printf "%03d" "$shard_num")-b$(printf "%020d" "$current_byte_offset").zfs.gz.age"
            
            # The uploader function is responsible for checking if the shard exists and skipping if needed.
            upload_shard_to_all_providers "$shard_file" "$output_file" "$dataset_path" "$agekey" "$shard_num"

            current_byte_offset=$((current_byte_offset + bytes_read))
            shard_num=$((shard_num + 1))

            if [[ $bytes_read -lt $dynamic_shard_size_bytes ]]; then
                break # Last shard was partial, so we are done.
            fi
        done
        wait # Wait for any backgrounded uploads to finish
    }

    echo "$(date '+%Y-%m-%d %H:%M:%S')     âœ“ Streaming backup process complete."
}

upload_shard_to_all_providers() {
    local shard_file="$1"
    local output_file="$2"
    local dataset_path="$3"
    local agekey="$4"
    local shard_num="$5"
    
    local shard_size=$(stat -c%s "$shard_file" 2>/dev/null || echo "0")
    echo "$(date '+%Y-%m-%d %H:%M:%S')       Processing shard $shard_num ($(numfmt --to=iec $shard_size)) to ${#SELECTED_PROVIDERS[@]} providers..."
    
    # Loop through all providers sequentially
    for prov in "${SELECTED_PROVIDERS[@]}"; do
        local rclone_remote="${PROVIDERS[$prov]}"
        local provider_name="${PROVIDER_NAMES[$prov]}"
        
        # Check if this shard already exists on this provider
        if rclone lsf "${rclone_remote}/${dataset_path}/" 2>/dev/null | grep -q "^${output_file}$"; then
            echo "$(date '+%Y-%m-%d %H:%M:%S')         âœ“ Shard $shard_num already exists on $provider_name"
        else
            # Upload to this provider
            upload_shard_to_provider "$shard_file" "$output_file" "$rclone_remote" "$dataset_path" "$agekey" "$provider_name" "$shard_num"
            if [[ $? -ne 0 ]]; then
                echo "$(date '+%Y-%m-%d %H:%M:%S')         âœ— Upload failed for shard $shard_num to $provider_name"
                # Continue with other providers even if one fails
            fi
        fi
    done
    
    # Clean up the temporary shard file
    rm -f "$shard_file"
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')       âœ“ Shard $shard_num completed on all providers"
}

upload_shard_to_provider() {
    local shard_file="$1"
    local output_file="$2"
    local rclone_remote="$3"
    local dataset_path="$4"
    local agekey="$5"
    local provider_name="$6"
    local shard_num="$7"
    
    echo "$(date '+%Y-%m-%d %H:%M:%S')         â†’ Uploading shard $shard_num to $provider_name..."
    
    # Stream shard: compress -> encrypt -> progress -> upload
    if gzip < "$shard_file" | age -r "$agekey" | rclone --progress rcat "${rclone_remote}/${dataset_path}/${output_file}"; then
        echo "$(date '+%Y-%m-%d %H:%M:%S')         âœ“ Uploaded shard $shard_num to $provider_name"
        return 0
    else
        echo "$(date '+%Y-%m-%d %H:%M:%S')         âœ— Failed to upload shard $shard_num to $provider_name"
        return 1
    fi
}

cleanup_old_snapshots() {
    local dataset="$1"
    echo "$(date '+%Y-%m-%d %H:%M:%S') [+] Cleaning up old snapshots for $dataset"
    
    zfs list -H -t snapshot -o name,creation -s creation | grep "^${dataset}@${SNAP_PREFIX}-" | while read snap creation; do
        local snap_time=$(echo "$snap" | awk -F@ '{print $2}' | sed 's/.*-//')
        local snap_date=$(date -d "${snap_time:0:8} ${snap_time:8:2}:${snap_time:10:2}:${snap_time:12:2}" +%s 2>/dev/null || continue)
        local now=$(date +%s)
        local age_days=$(( (now - snap_date) / 86400 ))
        
        if (( age_days > RETENTION_DAYS )); then
            echo "$(date '+%Y-%m-%d %H:%M:%S')     Destroying old snapshot $snap (${age_days} days old)"
            zfs destroy "$snap"
        fi
    done
}

# === END BACKUP FUNCTIONS ===

# === RESTORE FUNCTIONS ===

run_restore() {
    local dataset_arg="$1"
    local provider="${2:-auto}"
    local as_arg="$3"
    
    # Parse dataset@backup syntax
    if [[ "$dataset_arg" =~ @ ]]; then
        local source_dataset="${dataset_arg%@*}"
        local backup_prefix="${dataset_arg#*@}"
        
        # ZFS semantics: dataset and dataset@latest are identical
        if [[ "$backup_prefix" == "latest" ]]; then
            echo "Note: dataset@latest is the same as dataset (full dataset state)"
        fi
    else
        local source_dataset="$dataset_arg"
        local backup_prefix="latest"
    fi
    
    # Set target dataset from --as or use original name
    local target_dataset
    if [[ -n "$as_arg" ]]; then
        target_dataset="$as_arg"
    else
        target_dataset="$source_dataset"
    fi
    
    # Validate age key
    if [[ ! -f "$AGE_PRIVATE_KEY_FILE" ]]; then
        echo "ERROR: Age private key not found at $AGE_PRIVATE_KEY_FILE"
        exit 1
    fi
    
    # Provider selection
    case "$provider" in
        auto)
            # Try all providers, prefer backblaze
            echo "Auto mode: Testing all providers..."
            local dataset_path="${source_dataset//\//_}"
            local selected_remote=""
            local provider_name=""
            
            # Try providers in order of preference
            for prov in backblaze scaleway $(printf '%s\n' "${!PROVIDERS[@]}" | grep -v -E '^(backblaze|scaleway)$' | sort); do
                local remote="${PROVIDERS[$prov]}"
                if rclone lsf "${remote}/${dataset_path}/" 2>/dev/null | grep -q "^.*-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9].zfs.gz.age$"; then
                    selected_remote="$remote"
                    provider_name="${PROVIDER_NAMES[$prov]} - auto-selected"
                    echo "Found backups on $prov"
                    break
                fi
            done
            
            if [[ -z "$selected_remote" ]]; then
                echo "ERROR: No backups found on any provider"
                echo "Available providers: ${!PROVIDERS[*]}"
                exit 1
            fi
            ;;
        *)
            # Handle legacy aliases
            case "$provider" in
                bb) provider="backblaze" ;;
                scw) provider="scaleway" ;;
            esac
            
            if [[ -z "${PROVIDERS[$provider]:-}" ]]; then
                echo "ERROR: Unknown provider '$provider'"
                echo "Available providers: ${!PROVIDERS[*]}"
                exit 1
            fi
            
            selected_remote="${PROVIDERS[$provider]}"
            provider_name="${PROVIDER_NAMES[$provider]}"
            ;;
    esac
    
    # Setup variables
    local dataset_path="${source_dataset//\//_}"
    local remote_path="${selected_remote}/${dataset_path}"
    
    echo "=== Offsite Restore ==="
    echo "Provider: $provider_name"
    echo "Source dataset: $source_dataset"
    echo "Target dataset: $target_dataset"
    echo "Remote path: $remote_path"
    echo ""
    
    # Backup selection
    if [[ "$backup_prefix" == "latest" ]] || [[ "$backup_prefix" == "latest-full" ]]; then
        echo "Discovering available backups..."
        
        # Get all backup prefixes for this dataset (new byte offset format)
        local all_backups=($(rclone lsf "${remote_path}/" 2>/dev/null | grep -E '^.*-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9].zfs.gz.age$' | sed 's/-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9].zfs.gz.age$//' | sort -u || true))
        
        if [[ ${#all_backups[@]} -eq 0 ]]; then
            echo "ERROR: No backups found for dataset $source_dataset"
            exit 1
        fi
        
        if [[ "$backup_prefix" == "latest-full" ]]; then
            # In new format, all backups are "full" - just get the latest
            backup_prefix="${all_backups[-1]}"
            echo "Selected latest backup: $backup_prefix"
        else
            # Find latest backup
            backup_prefix="${all_backups[-1]}"
            echo "Selected latest backup: $backup_prefix"
        fi
        
        echo "Available backups for $source_dataset:"
        printf '%s\n' "${all_backups[@]}" | tail -5
        echo ""
    else
        # User specified a specific backup - need to find the actual backup prefix
        echo "Searching for backup matching: $backup_prefix"
        
        # Get all backup prefixes for this dataset (new byte offset format)
        local all_backups=($(rclone lsf "${remote_path}/" 2>/dev/null | grep -E '^.*-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9].zfs.gz.age$' | sed 's/-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9].zfs.gz.age$//' | sort -u || true))
        
        if [[ ${#all_backups[@]} -eq 0 ]]; then
            echo "ERROR: No backups found for dataset $source_dataset"
            exit 1
        fi
        
        # Look for backup that matches the specified snapshot name
        local found_backup=""
        for backup in "${all_backups[@]}"; do
            if [[ "$backup" == "${source_dataset//\//_}:${backup_prefix}" ]]; then
                found_backup="$backup"
                break
            fi
        done
        
        if [[ -z "$found_backup" ]]; then
            echo "ERROR: No backup found matching snapshot: $backup_prefix"
            echo "Available backups:"
            printf '%s\n' "${all_backups[@]}"
            exit 1
        fi
        
        backup_prefix="$found_backup"
        echo "Found matching backup: $backup_prefix"
    fi
    
    echo "Final backup selection: $backup_prefix"
    echo ""
    
    # Discover shards
    echo "Discovering backup shards..."
    local shard_files=($(rclone lsf "${remote_path}/" 2>/dev/null | grep "^${backup_prefix}-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9].zfs.gz.age$" | sort))
    
    if [[ ${#shard_files[@]} -eq 0 ]]; then
        echo "ERROR: No shards found for backup prefix: ${backup_prefix}"
        echo ""
        echo "Available backups for ${source_dataset}:"
        rclone lsf "${remote_path}/" 2>/dev/null | grep -E '^.*-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\.zfs\.gz\.age$' | sed 's/-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\.zfs\.gz\.age$//' | sort -u || echo "No backups found"
        exit 1
    fi
    
    echo "Found ${#shard_files[@]} shards: ${shard_files[0]} ... ${shard_files[-1]}"
    
    # Connection test
    echo "Testing connection to $provider_name..."
    if ! rclone lsd "${selected_remote%:*}:" >/dev/null 2>&1; then
        echo "ERROR: Cannot connect to $provider_name"
        exit 1
    fi
    echo "âœ“ Connected to $provider_name"
    
    # Target dataset validation
    if zfs list "$target_dataset" >/dev/null 2>&1; then
        # Check if it's a pool or dataset
        local target_type=$(zfs get -H -o value type "$target_dataset" 2>/dev/null)
        
        if [[ "$target_type" == "filesystem" ]]; then
            echo "WARNING: Target dataset $target_dataset already exists"
            
            # Check if running in background (no TTY)
            if [[ ! -t 0 ]]; then
                echo "ERROR: Cannot prompt for confirmation when running in background"
                echo "Target dataset already exists. Use --as to specify a different target name"
                exit 1
            fi
            
            read -p "Do you want to destroy and recreate it? (y/N): " -n 1 -r
            echo
            if [[ ! $REPLY =~ ^[Yy]$ ]]; then
                echo "Restore cancelled"
                echo "Suggestion: Use --as to specify a different target name"
                exit 1
            fi
            echo "Destroying existing dataset..."
            zfs destroy -r "$target_dataset"
        else
            echo "ERROR: Target $target_dataset exists but is not a filesystem"
            echo "Type: $target_type"
            echo "Use --as to specify a different target name"
            exit 1
        fi
    else
        # Check if parent exists for dataset creation
        local parent_dataset=$(dirname "$target_dataset" | sed 's|/.*||' | head -1)
        local dataset_path=$(echo "$target_dataset" | sed 's|^[^/]*/||')
        
        # If target contains '/', check parent exists
        if [[ "$target_dataset" =~ / ]]; then
            local parent_path=$(echo "$target_dataset" | sed 's|/[^/]*$||')
            if ! zfs list "$parent_path" >/dev/null 2>&1; then
                echo "ERROR: Parent dataset $parent_path does not exist"
                echo "Create parent first or use --as with an existing parent"
                exit 1
            fi
        fi
    fi
    
    # Restore process
    local work_dir="${TEMP_DIR}/offsite-restore-$(date +%s)"
    mkdir -p "$work_dir"
    cd "$work_dir"
    
    echo ""
    echo "Starting restore process..."
    echo "Working directory: $work_dir"
    
    # Ensure target dataset mountpoint exists
    echo "Preparing target dataset: $target_dataset"
    
    # Get the expected mountpoint for the target dataset
    local mountpoint
    if zfs list -H -o mountpoint "$target_dataset" 2>/dev/null; then
        # Dataset exists, get its mountpoint
        mountpoint=$(zfs list -H -o mountpoint "$target_dataset" 2>/dev/null)
        echo "  Target dataset already exists with mountpoint: $mountpoint"
    else
        # Dataset doesn't exist, predict the mountpoint
        # ZFS typically inherits from parent or uses /<pool>/<dataset>
        local pool_name="${target_dataset%%/*}"
        local dataset_path="${target_dataset#*/}"
        
        # Check if pool exists and get its mountpoint
        if zfs list -H -o mountpoint "$pool_name" 2>/dev/null; then
            local pool_mountpoint=$(zfs list -H -o mountpoint "$pool_name" 2>/dev/null)
            if [[ "$pool_mountpoint" == "none" ]] || [[ "$pool_mountpoint" == "-" ]]; then
                # Pool has no mountpoint, use standard ZFS path
                mountpoint="/$target_dataset"
            else
                # Pool has mountpoint, build path from it
                if [[ -n "$dataset_path" ]]; then
                    mountpoint="$pool_mountpoint/$dataset_path"
                else
                    mountpoint="$pool_mountpoint"
                fi
            fi
        else
            echo "ERROR: ZFS pool '$pool_name' not found"
            exit 1
        fi
        
        echo "  Target dataset will be created with mountpoint: $mountpoint"
    fi
    
    # Create mountpoint directory if it doesn't exist
    if [[ "$mountpoint" != "none" ]] && [[ "$mountpoint" != "-" ]] && [[ ! -d "$mountpoint" ]]; then
        echo "  Creating mountpoint directory: $mountpoint"
        if sudo mkdir -p "$mountpoint"; then
            echo "  âœ“ Mountpoint created successfully"
        else
            echo "  âœ— Failed to create mountpoint directory"
            exit 1
        fi
    fi
    
    # Stream to ZFS receive with individual progress bars and pipeline buffer
    echo "Step 1: Streaming ${#shard_files[@]} shards to ZFS receive..."
    echo ""
    
    # Sort shards by byte offset to ensure correct order
    local sorted_shards=()
    for shard_file in "${shard_files[@]}"; do
        local offset=$(echo "$shard_file" | sed 's/.*-s[0-9][0-9][0-9]-b\([0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\)\.zfs\.gz\.age$/\1/')
        sorted_shards+=("$offset:$shard_file")
    done
    
    # Sort by byte offset (numeric sort on the offset part)
    IFS=$'\n' sorted_shards=($(sort -t: -k1,1n <<< "${sorted_shards[*]}"))
    
    echo "  Streaming ${#sorted_shards[@]} shards with individual progress bars..."
    echo ""
    
    local total_shards=${#sorted_shards[@]}
    
    # Stream shards directly to ZFS receive - proven working approach
    {
        for entry in "${sorted_shards[@]}"; do
            local shard_file="${entry#*:}"
            echo "    Processing: $shard_file" >&2
            
            # Stream: download â†’ decrypt â†’ decompress
            rclone cat "${remote_path}/${shard_file}" | age -d -i "$AGE_PRIVATE_KEY_FILE" | gunzip
        done
    } | zfs recv -F "$target_dataset"
    
    local zfs_recv_result=$?
    
    if [[ $zfs_recv_result -eq 0 ]]; then
        echo "  âœ“ ZFS receive completed successfully"
    else
        echo "  âœ— ZFS receive failed with exit code: $zfs_recv_result"
        return $zfs_recv_result
    fi
    
    # Cleanup
    cd /
    rm -rf "$work_dir"
    
    echo ""
    echo "âœ… Restore complete!"
    echo ""
    echo "Source: $source_dataset ($backup_prefix)"
    echo "Target: $target_dataset"
    echo "Shards processed: ${#shard_files[@]}"
    echo "Provider: $provider_name"
    echo ""
    echo "You can now:"
    echo "  - Mount: zfs mount $target_dataset"
    echo "  - List snapshots: zfs list -t snapshot -r $target_dataset"
    echo "  - Access data at: $(zfs get -H -o value mountpoint $target_dataset 2>/dev/null || echo 'mount point')"
}

# === END RESTORE FUNCTIONS ===

# === LIST FUNCTIONS ===

run_list() {
    local dataset_arg="$1"
    local provider="${2:-all}"
    
    echo "=== Offsite Backup List ==="
    echo ""
    
    # Provider selection (same logic as backup)
    if [[ "$provider" == "all" ]]; then
        SELECTED_PROVIDERS=($(printf '%s\n' "${!PROVIDERS[@]}" | sort))
    else
        case "$provider" in
            bb) provider="backblaze" ;;
            scw) provider="scaleway" ;;
        esac
        
        if [[ -z "${PROVIDERS[$provider]:-}" ]]; then
            echo "ERROR: Unknown provider '$provider'"
            echo "Available providers: ${\!PROVIDERS[*]}"
            exit 1
        fi
        
        SELECTED_PROVIDERS=("$provider")
    fi
    
    # List backups for each provider
    for prov in "${SELECTED_PROVIDERS[@]}"; do
        rclone_remote="${PROVIDERS[$prov]}"
        provider_name="${PROVIDER_NAMES[$prov]}"
        
        echo "ðŸ“¡ $provider_name ($prov)"
        echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        
        if [[ -n "$dataset_arg" ]]; then
            # List specific dataset
            list_dataset_backups "$rclone_remote" "$dataset_arg"
        else
            # List all datasets
            list_all_backups "$rclone_remote"
        fi
        
        echo ""
    done
}

list_dataset_backups() {
    local rclone_remote="$1"
    local dataset="$2"
    local dataset_path="${dataset//\//_}"
    
    # Get all backup prefixes for this dataset (new byte offset format)
    local backup_files=($(rclone lsf "${rclone_remote}/${dataset_path}/" 2>/dev/null | grep -E "^.*-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\.zfs\.gz\.age$" | sort || true))
    
    if [[ ${#backup_files[@]} -eq 0 ]]; then
        echo "  No backups found for $dataset"
        return
    fi
    
    # Extract unique backup prefixes (remove shard suffix from new format)
    local backup_prefixes=($(printf '%s\n' "${backup_files[@]}" | sed -E 's/-s[0-9][0-9][0-9]-b[0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9][0-9]\.zfs\.gz\.age$//' | sort -u))
    
    echo "  Dataset: $dataset"
    echo ""
    
    for prefix in "${backup_prefixes[@]}"; do
        # Count shards for this backup (new byte offset format)
        local shard_count=$(printf '%s\n' "${backup_files[@]}" | grep "^${prefix}-s[0-9][0-9][0-9]-b" | wc -l)
        
        # Check if backup is complete (simplified for now)
        local status="âŒ Incomplete" 
        # TODO: Re-enable completion check when function scope issue is resolved
        # if is_backup_complete "$rclone_remote" "$dataset_path" "$prefix"; then
        #     status="âœ… Complete"
        # fi
        
        # Display backup info (new format is dataset:snapshot)
        echo "    $status $prefix ($shard_count shards)"
    done
}

list_all_backups() {
    local rclone_remote="$1"
    
    # Get all dataset directories
    local datasets=($(rclone lsd "${rclone_remote}/" 2>/dev/null | awk '{print $NF}' | grep -v "^$" || true))
    
    if [[ ${#datasets[@]} -eq 0 ]]; then
        echo "  No backups found"
        return
    fi
    
    for dataset_path in "${datasets[@]}"; do
        # Convert path back to dataset name
        local dataset="${dataset_path//_//}"
        list_dataset_backups "$rclone_remote" "$dataset"
        echo ""
    done
}

# === END LIST FUNCTIONS ===
